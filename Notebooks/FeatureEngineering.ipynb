{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering."
      ],
      "metadata": {
        "id": "WPj6EBAy0YEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering. Handle datasets in order to make sure test sets do have new relations/entities"
      ],
      "metadata": {
        "id": "-5G5cLK_0awH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PKZGpjj0VUN"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "Inductive relation-based partitioning for Knowledge Graph datasets.\n",
        "Generates NL-25 / NL-50 / NL-75 / NL-100 splits where relations are unseen in train.\n",
        "\n",
        "Assumptions:\n",
        "- Input datasets follow:\n",
        "  data/DATASET/train.txt\n",
        "  data/DATASET/valid.txt\n",
        "  data/DATASET/test.txt\n",
        "- Triplets are tab-separated: h \\t r \\t t\n",
        "- Entities may appear in any split (allowed)\n",
        "- Relations selected as \"new\" do NOT appear in train\n",
        "- valid and test contain ONLY new relations\n",
        "- Reproducible via fixed seed\n",
        "- No external deps beyond Python stdlib (+ optional numpy, not required)\n",
        "\n",
        "Author: you + ChatGPT\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "BASE_DATA_DIR = \"data\"\n",
        "DATASET_NAME = \"WN18RR\"      # change if needed\n",
        "SEED = 42\n",
        "\n",
        "ALPHAS = {\n",
        "    \"NL-25\": 0.25,\n",
        "    \"NL-50\": 0.50,\n",
        "    \"NL-75\": 0.75,\n",
        "    \"NL-100\": 1.00,\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# UTILS\n",
        "# =========================\n",
        "def read_triples(path):\n",
        "    triples = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            h, r, t = line.split(\"\\t\")\n",
        "            triples.append((h, r, t))\n",
        "    return triples\n",
        "\n",
        "\n",
        "def write_triples(path, triples):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for h, r, t in triples:\n",
        "            f.write(f\"{h}\\t{r}\\t{t}\\n\")\n",
        "\n",
        "\n",
        "def ensure_dir(path):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# MAIN LOGIC\n",
        "# =========================\n",
        "def main():\n",
        "    random.seed(SEED)\n",
        "\n",
        "    dataset_dir = os.path.join(BASE_DATA_DIR, DATASET_NAME)\n",
        "    train_path = os.path.join(dataset_dir, \"train.txt\")\n",
        "    valid_path = os.path.join(dataset_dir, \"valid.txt\")\n",
        "    test_path  = os.path.join(dataset_dir, \"test.txt\")\n",
        "\n",
        "    print(f\"\\n[INFO] Loading base dataset: {DATASET_NAME}\")\n",
        "\n",
        "    train_triples = read_triples(train_path)\n",
        "    valid_triples = read_triples(valid_path)\n",
        "    test_triples  = read_triples(test_path)\n",
        "\n",
        "    all_triples = train_triples + valid_triples + test_triples\n",
        "\n",
        "    # Group triples by relation\n",
        "    rel2triples = defaultdict(list)\n",
        "    for h, r, t in all_triples:\n",
        "        rel2triples[r].append((h, r, t))\n",
        "\n",
        "    all_relations = sorted(rel2triples.keys())\n",
        "    num_relations = len(all_relations)\n",
        "\n",
        "    print(f\"[STATS] Total triples      : {len(all_triples)}\")\n",
        "    print(f\"[STATS] Total relations    : {num_relations}\")\n",
        "\n",
        "    for split_name, alpha in ALPHAS.items():\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(f\"[SPLIT] Generating {split_name} (alpha={alpha})\")\n",
        "\n",
        "        num_new_rel = int(round(num_relations * alpha))\n",
        "\n",
        "        if num_new_rel > num_relations:\n",
        "            num_new_rel = num_relations\n",
        "\n",
        "        shuffled_relations = all_relations[:]\n",
        "        random.shuffle(shuffled_relations)\n",
        "\n",
        "        new_relations = set(shuffled_relations[:num_new_rel])\n",
        "        old_relations = set(shuffled_relations[num_new_rel:])\n",
        "\n",
        "        # Build splits\n",
        "        train_split = []\n",
        "        valid_split = []\n",
        "        test_split  = []\n",
        "\n",
        "        for r in old_relations:\n",
        "            train_split.extend(rel2triples[r])\n",
        "\n",
        "        new_relation_triples = []\n",
        "        for r in new_relations:\n",
        "            new_relation_triples.extend(rel2triples[r])\n",
        "\n",
        "        # Split new-relation triples into valid/test (50/50)\n",
        "        random.shuffle(new_relation_triples)\n",
        "        mid = len(new_relation_triples) // 2\n",
        "        valid_split = new_relation_triples[:mid]\n",
        "        test_split  = new_relation_triples[mid:]\n",
        "\n",
        "        # Safety checks\n",
        "        train_rels = {r for _, r, _ in train_split}\n",
        "        valid_rels = {r for _, r, _ in valid_split}\n",
        "        test_rels  = {r for _, r, _ in test_split}\n",
        "\n",
        "        assert train_rels.isdisjoint(new_relations), \"Leakage: new relations in train!\"\n",
        "        assert valid_rels.issubset(new_relations), \"Invalid relation in valid!\"\n",
        "        assert test_rels.issubset(new_relations), \"Invalid relation in test!\"\n",
        "\n",
        "        # Output directory\n",
        "        out_dir = os.path.join(dataset_dir, split_name)\n",
        "        ensure_dir(out_dir)\n",
        "\n",
        "        write_triples(os.path.join(out_dir, \"train.txt\"), train_split)\n",
        "        write_triples(os.path.join(out_dir, \"valid.txt\"), valid_split)\n",
        "        write_triples(os.path.join(out_dir, \"test.txt\"),  test_split)\n",
        "\n",
        "        # Report\n",
        "        print(f\"[STATS] #new relations     : {len(new_relations)}\")\n",
        "        print(f\"[STATS] train triples     : {len(train_split)}\")\n",
        "        print(f\"[STATS] valid triples     : {len(valid_split)}\")\n",
        "        print(f\"[STATS] test  triples     : {len(test_split)}\")\n",
        "        print(f\"[PATH ] Written to        : {out_dir}\")\n",
        "\n",
        "    print(\"\\n[DONE] All NL-* splits generated successfully.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}