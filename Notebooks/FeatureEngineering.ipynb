{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering."
      ],
      "metadata": {
        "id": "WPj6EBAy0YEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering. Handle datasets in order to make sure test sets do have new relations/entities"
      ],
      "metadata": {
        "id": "-5G5cLK_0awH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ## 1. Environment and GPU sanity check"
      ],
      "metadata": {
        "id": "Mj6g11A-IjnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU check for future pipeline\n",
        "import torch\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA Version:\", torch.version.cuda)\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  print(\"------------No GPU. Set Runtime → Change runtime type → GPU------------\")\n",
        "\n",
        "try:\n",
        "    import torch_geometric\n",
        "    print(\"Torch Geometric:\", torch_geometric.__version__)\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Torch Geometric not found. Installing\")\n",
        "    torch_version = torch.__version__.split(\"+\")[0]\n",
        "    cuda_version = torch.version.cuda.replace(\".\", \"\")\n",
        "\n",
        "    !pip install -q pyg-lib torch-scatter torch-sparse torch-cluster torch-spline-conv \\\n",
        "        -f https://data.pyg.org/whl/torch-{torch_version}+cu{cuda_version}.html\n",
        "\n",
        "    !pip install -q torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eismSo1BBGoH",
        "outputId": "05ec9a99-a79a-4a17-869b-500ffcf6827a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "CUDA Version: 12.6\n",
            "Torch Geometric: 2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataset download and normalization"
      ],
      "metadata": {
        "id": "7pI_W7KtImWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -r raw_data data"
      ],
      "metadata": {
        "id": "xFn3_3wAO-xX"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Dataset download & normalization\n",
        "# =========================\n",
        "\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import pandas as pd\n",
        "from torch_geometric.datasets import WordNet18RR, FB15k_237\n",
        "\n",
        "# -------------------------\n",
        "# Paths\n",
        "# -------------------------\n",
        "RAW_DIR  = Path(\"./raw_data\")      # Raw / potentially dirty datasets\n",
        "DATA_DIR = Path(\"./data/newlinks\") # Normalized datasets (h, r, t) for New Links\n",
        "\n",
        "RAW_DIR.mkdir(exist_ok=True)\n",
        "DATA_DIR.mkdir(parents=True,exist_ok=True)\n",
        "\n",
        "print(f\"RAW_DIR : {RAW_DIR.resolve()}\")\n",
        "print(f\"DATA_DIR: {DATA_DIR.resolve()}\")\n",
        "\n",
        "# -------------------------\n",
        "# Helpers\n",
        "# -------------------------\n",
        "def normalize_to_txt(src_path: Path, dst_path: Path):\n",
        "    \"\"\"\n",
        "    Read raw KG triple file src_path and saves first 3 columns\n",
        "    as head<TAB>rel<TAB>tail into dst_path.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(\n",
        "        src_path,\n",
        "        sep=None,\n",
        "        engine=\"python\",\n",
        "        header=None,\n",
        "        on_bad_lines=\"skip\"\n",
        "    )\n",
        "\n",
        "    if df.shape[1] < 3:\n",
        "        raise ValueError(\n",
        "            f\"[FORMAT ERROR] Invalid KG triple file: {src_path}\\n\"\n",
        "            f\"Detected columns: {df.shape[1]}\\n\"\n",
        "            \"Expected format: head, relation, tail, [optional extra columns]\"\n",
        "        )\n",
        "\n",
        "    df.iloc[:, :3].to_csv(dst_path, sep=\"\\t\", index=False, header=False)\n",
        "\n",
        "\n",
        "def pyg_dataset_to_standard(pyg_dataset, name: str):\n",
        "    \"\"\"\n",
        "    Normalize (tab) PyG raw files from raw\n",
        "    and saves as data/name/{train,valid,test}.txt\n",
        "    into data/name\n",
        "    \"\"\"\n",
        "    raw_dir = Path(pyg_dataset.raw_dir)\n",
        "    out_dir = DATA_DIR / name\n",
        "    out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    print(f\"\\nProcessing PyG dataset: {name}\")\n",
        "\n",
        "    file_map = {\n",
        "        \"train\": [\"train.txt\"],\n",
        "        \"valid\": [\"valid.txt\", \"valid.csv\"],\n",
        "        \"test\":  [\"test.txt\"]\n",
        "    }\n",
        "\n",
        "    for split, candidates in file_map.items():\n",
        "        for fname in candidates:\n",
        "            src = raw_dir / fname\n",
        "            if src.exists():\n",
        "                dst = out_dir / f\"{split}.txt\"\n",
        "                normalize_to_txt(src, dst)\n",
        "                print(f\"  -> {split}.txt\")\n",
        "                break\n",
        "        else:\n",
        "            print(f\"  [!] Missing split: {split}\")\n",
        "\n",
        "\n",
        "def download_file(url: str, dst: Path):\n",
        "    if dst.exists():\n",
        "        return\n",
        "    print(f\"Downloading {dst.name}...\")\n",
        "    r = requests.get(url)\n",
        "    r.raise_for_status()\n",
        "    dst.write_bytes(r.content)\n",
        "\n",
        "# -------------------------\n",
        "# PyG datasets\n",
        "# -------------------------\n",
        "print(\"\\n--- Downloading PyG datasets ---\")\n",
        "\n",
        "wn18rr = WordNet18RR(root=RAW_DIR / \"WordNet18RR\")\n",
        "pyg_dataset_to_standard(wn18rr, \"WN18RR\")\n",
        "\n",
        "fb237 = FB15k_237(root=RAW_DIR / \"FB15k-237\")\n",
        "pyg_dataset_to_standard(fb237, \"FB15k-237\")\n",
        "\n",
        "# -------------------------\n",
        "# External datasets\n",
        "# -------------------------\n",
        "print(\"\\n--- Downloading external datasets ---\")\n",
        "\n",
        "EXTERNAL_DATASETS = {\n",
        "    \"CoDEx-M\": \"https://raw.githubusercontent.com/tsafavi/codex/master/data/triples/codex-m/\",\n",
        "    \"WN11\":    \"https://raw.githubusercontent.com/KGCompletion/TransL/master/WN11/\",\n",
        "    \"FB13\":    \"https://raw.githubusercontent.com/KGCompletion/TransL/master/FB13/\",\n",
        "}\n",
        "\n",
        "for name, base_url in EXTERNAL_DATASETS.items():\n",
        "    raw_out = RAW_DIR / name\n",
        "    data_out = DATA_DIR / name\n",
        "    raw_out.mkdir(exist_ok=True)\n",
        "    data_out.mkdir(exist_ok=True)\n",
        "\n",
        "    print(f\"\\n{name}\")\n",
        "    for split in [\"train\", \"valid\", \"test\"]:\n",
        "        url = f\"{base_url}{split}.txt\"\n",
        "        raw_path = raw_out / f\"{split}.txt\"\n",
        "        data_path = data_out / f\"{split}.txt\"\n",
        "\n",
        "        download_file(url, raw_path)\n",
        "        normalize_to_txt(raw_path, data_path)\n",
        "        print(f\"  -> {split}.txt\")\n",
        "\n",
        "    if name != \"CoDEx-M\":\n",
        "      for split in [\"entity2id\", \"relation2id\"]:\n",
        "          url = f\"{base_url}{split}.txt\"\n",
        "          raw_path = raw_out / f\"{split}.txt\"\n",
        "          download_file(url, raw_path)\n",
        "\n",
        "print(\"\\n[DONE] All datasets downloaded and normalized.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4frQnX-Buym",
        "outputId": "38ef7d03-c3af-4a5d-cd83-283fa906b9d0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAW_DIR : /content/raw_data\n",
            "DATA_DIR: /content/data/newlinks\n",
            "\n",
            "--- Downloading PyG datasets ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/WN18RR/original/train.txt\n",
            "Downloading https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/WN18RR/original/valid.txt\n",
            "Downloading https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/WN18RR/original/test.txt\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing PyG dataset: WN18RR\n",
            "  -> train.txt\n",
            "  -> valid.txt\n",
            "  -> test.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/FB15k-237/train.txt\n",
            "Downloading https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/FB15k-237/valid.txt\n",
            "Downloading https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/FB15k-237/test.txt\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing PyG dataset: FB15k-237\n",
            "  -> train.txt\n",
            "  -> valid.txt\n",
            "  -> test.txt\n",
            "\n",
            "--- Downloading external datasets ---\n",
            "\n",
            "CoDEx-M\n",
            "Downloading train.txt...\n",
            "  -> train.txt\n",
            "Downloading valid.txt...\n",
            "  -> valid.txt\n",
            "Downloading test.txt...\n",
            "  -> test.txt\n",
            "\n",
            "WN11\n",
            "Downloading train.txt...\n",
            "  -> train.txt\n",
            "Downloading valid.txt...\n",
            "  -> valid.txt\n",
            "Downloading test.txt...\n",
            "  -> test.txt\n",
            "Downloading entity2id.txt...\n",
            "Downloading relation2id.txt...\n",
            "\n",
            "FB13\n",
            "Downloading train.txt...\n",
            "  -> train.txt\n",
            "Downloading valid.txt...\n",
            "  -> valid.txt\n",
            "Downloading test.txt...\n",
            "  -> test.txt\n",
            "Downloading entity2id.txt...\n",
            "Downloading relation2id.txt...\n",
            "\n",
            "[DONE] All datasets downloaded and normalized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Inductive relation-based splits (InGram setup)"
      ],
      "metadata": {
        "id": "zjt1eS4UIpv0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "3PKZGpjj0VUN"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Inductive relation-based splits (NL-*)\n",
        "# =========================\n",
        "\n",
        "from pathlib import Path\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "SEED = 42\n",
        "\n",
        "ALPHAS = {\n",
        "    \"NL-25\": 0.25,\n",
        "    \"NL-50\": 0.50,\n",
        "    \"NL-75\": 0.75,\n",
        "    \"NL-100\": 1.00,\n",
        "}\n",
        "\n",
        "random.seed(SEED)\n",
        "\n",
        "# -------------------------\n",
        "# IO helpers\n",
        "# -------------------------\n",
        "def read_triples(path: Path):\n",
        "    triples = []\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            h, r, t = line.rstrip(\"\\n\").split(\"\\t\")\n",
        "            triples.append((h, r, t))\n",
        "    return triples\n",
        "\n",
        "\n",
        "def write_triples(path: Path, triples):\n",
        "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        for h, r, t in triples:\n",
        "            f.write(f\"{h}\\t{r}\\t{t}\\n\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Core logic\n",
        "# -------------------------\n",
        "def generate_inductive_splits(dataset_dir: Path):\n",
        "    \"\"\"\n",
        "    Generate inductive relation-based splits (NL-*) for a dataset directory.\n",
        "\n",
        "    The input directory must contain:\n",
        "        train.txt\n",
        "        valid.txt\n",
        "        test.txt\n",
        "\n",
        "    The function creates, inside the same directory:\n",
        "        NL-25/, NL-50/, NL-75/, NL-100/\n",
        "    each containing train/valid/test splits where relations in valid/test\n",
        "    are completely unseen during training.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset_dir : Path\n",
        "        Path to a dataset directory under BASE_DATA_DIR.\n",
        "    \"\"\"\n",
        "    train_path = dataset_dir / \"train.txt\"\n",
        "    valid_path = dataset_dir / \"valid.txt\"\n",
        "    test_path  = dataset_dir / \"test.txt\"\n",
        "\n",
        "    if not (train_path.exists() and valid_path.exists() and test_path.exists()):\n",
        "        print(f\"[SKIP] {dataset_dir.name}: missing train/valid/test files\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n[DATASET] {dataset_dir.name}\")\n",
        "\n",
        "    train = read_triples(train_path)\n",
        "    valid = read_triples(valid_path)\n",
        "    test  = read_triples(test_path)\n",
        "\n",
        "    all_triples = train + valid + test\n",
        "\n",
        "    # Group triples by relation\n",
        "    rel2triples = defaultdict(list)\n",
        "    for h, r, t in all_triples:\n",
        "        rel2triples[r].append((h, r, t))\n",
        "\n",
        "    relations = list(rel2triples.keys())\n",
        "    num_relations = len(relations)\n",
        "\n",
        "    print(f\"  Total relations : {num_relations}\")\n",
        "    print(f\"  Total triples   : {len(all_triples)}\")\n",
        "\n",
        "    for split_name, alpha in ALPHAS.items():\n",
        "        n_new = int(round(num_relations * alpha))\n",
        "\n",
        "        shuffled = relations[:]\n",
        "        random.shuffle(shuffled)\n",
        "\n",
        "        new_rels = set(shuffled[:n_new])\n",
        "        old_rels = set(shuffled[n_new:])\n",
        "\n",
        "        train_split = []\n",
        "        for r in old_rels:\n",
        "            train_split.extend(rel2triples[r])\n",
        "\n",
        "        new_triples = []\n",
        "        for r in new_rels:\n",
        "            new_triples.extend(rel2triples[r])\n",
        "\n",
        "        random.shuffle(new_triples)\n",
        "        mid = len(new_triples) // 2\n",
        "        valid_split = new_triples[:mid]\n",
        "        test_split  = new_triples[mid:]\n",
        "\n",
        "        # Safety checks\n",
        "        assert {r for _, r, _ in train_split}.isdisjoint(new_rels)\n",
        "        assert {r for _, r, _ in valid_split}.issubset(new_rels)\n",
        "        assert {r for _, r, _ in test_split}.issubset(new_rels)\n",
        "\n",
        "        out_dir = dataset_dir / split_name\n",
        "        out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        write_triples(out_dir / \"train.txt\", train_split)\n",
        "        write_triples(out_dir / \"valid.txt\", valid_split)\n",
        "        write_triples(out_dir / \"test.txt\",  test_split)\n",
        "\n",
        "        print(\n",
        "            f\"  [{split_name}] \"\n",
        "            f\"new_rel={len(new_rels)} | \"\n",
        "            f\"train={len(train_split)} | \"\n",
        "            f\"valid={len(valid_split)} | \"\n",
        "            f\"test={len(test_split)}\"\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Run for all datasets\n",
        "# -------------------------\n",
        "print(\"\\n=== Generating inductive splits for all datasets ===\")\n",
        "\n",
        "for dataset_dir in DATA_DIR.iterdir():\n",
        "    if dataset_dir.is_dir():\n",
        "        generate_inductive_splits(dataset_dir)\n",
        "\n",
        "print(\"\\n[DONE] All NL-* splits generated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b5Rk6RiTudv",
        "outputId": "43443997-6c97-42b3-f020-5cfb312a12ea"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Generating inductive splits for all datasets ===\n",
            "\n",
            "[DATASET] CoDEx-M\n",
            "  Total relations : 51\n",
            "  Total triples   : 206205\n",
            "  [NL-25] new_rel=13 | train=173839 | valid=16183 | test=16183\n",
            "  [NL-50] new_rel=26 | train=137278 | valid=34463 | test=34464\n",
            "  [NL-75] new_rel=38 | train=97895 | valid=54155 | test=54155\n",
            "  [NL-100] new_rel=51 | train=0 | valid=103102 | test=103103\n",
            "\n",
            "[DATASET] FB15k-237\n",
            "  Total relations : 237\n",
            "  Total triples   : 310116\n",
            "  [NL-25] new_rel=59 | train=220712 | valid=44702 | test=44702\n",
            "  [NL-50] new_rel=118 | train=126564 | valid=91776 | test=91776\n",
            "  [NL-75] new_rel=178 | train=75589 | valid=117263 | test=117264\n",
            "  [NL-100] new_rel=237 | train=0 | valid=155058 | test=155058\n",
            "\n",
            "[DATASET] FB13\n",
            "  Total relations : 13\n",
            "  Total triples   : 375514\n",
            "  [NL-25] new_rel=3 | train=317509 | valid=29002 | test=29003\n",
            "  [NL-50] new_rel=6 | train=159199 | valid=108157 | test=108158\n",
            "  [NL-75] new_rel=10 | train=135632 | valid=119941 | test=119941\n",
            "  [NL-100] new_rel=13 | train=0 | valid=187757 | test=187757\n",
            "\n",
            "[DATASET] WN18RR\n",
            "  Total relations : 11\n",
            "  Total triples   : 93003\n",
            "  [NL-25] new_rel=3 | train=52533 | valid=20235 | test=20235\n",
            "  [NL-50] new_rel=6 | train=46366 | valid=23318 | test=23319\n",
            "  [NL-75] new_rel=8 | train=5406 | valid=43798 | test=43799\n",
            "  [NL-100] new_rel=11 | train=0 | valid=46501 | test=46502\n",
            "\n",
            "[DATASET] WN11\n",
            "  Total relations : 11\n",
            "  Total triples   : 138887\n",
            "  [NL-25] new_rel=3 | train=55644 | valid=41621 | test=41622\n",
            "  [NL-50] new_rel=6 | train=17247 | valid=60820 | test=60820\n",
            "  [NL-75] new_rel=8 | train=49919 | valid=44484 | test=44484\n",
            "  [NL-100] new_rel=11 | train=0 | valid=69443 | test=69444\n",
            "\n",
            "[DONE] All NL-* splits generated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "UNSEEN_RATIO = 0.20   # % de entidades OOKB\n",
        "OOKB_DIR = Path(\"./data/newentities\")\n",
        "OOKB_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# OOKB logic\n",
        "# -------------------------\n",
        "def generate_ookb_splits(dataset_dir: Path):\n",
        "    train_path = dataset_dir / \"train.txt\"\n",
        "    valid_path = dataset_dir / \"valid.txt\"\n",
        "    test_path  = dataset_dir / \"test.txt\"\n",
        "\n",
        "    if not (train_path.exists() and valid_path.exists() and test_path.exists()):\n",
        "        print(f\"[SKIP] {dataset_dir.name}: missing train/valid/test\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n[OOKB DATASET] {dataset_dir.name}\")\n",
        "\n",
        "    train = read_triples(train_path)\n",
        "    valid = read_triples(valid_path)\n",
        "    test  = read_triples(test_path)\n",
        "\n",
        "    all_triples = train + valid + test\n",
        "\n",
        "    # -------- collect entities & relations --------\n",
        "    entities = set()\n",
        "    relations = set()\n",
        "    for h, r, t in all_triples:\n",
        "        entities.update([h, t])\n",
        "        relations.add(r)\n",
        "\n",
        "    entities  = list(entities)\n",
        "    relations = list(relations)\n",
        "\n",
        "    # -------- select unseen entities --------\n",
        "    random.shuffle(entities)\n",
        "    n_unseen = int(round(len(entities) * UNSEEN_RATIO))\n",
        "    unseen_entities = set(entities[:n_unseen])\n",
        "\n",
        "    print(f\"  entities={len(entities)} | unseen={len(unseen_entities)}\")\n",
        "\n",
        "    # -------- split triples --------\n",
        "    train_split = []\n",
        "    new_triples = []\n",
        "\n",
        "    for h, r, t in all_triples:\n",
        "        if h in unseen_entities or t in unseen_entities:\n",
        "            new_triples.append((h, r, t))\n",
        "        else:\n",
        "            train_split.append((h, r, t))\n",
        "\n",
        "    random.shuffle(new_triples)\n",
        "    mid = len(new_triples) // 2\n",
        "    valid_split = new_triples[:mid]\n",
        "    test_split  = new_triples[mid:]\n",
        "\n",
        "    # -------- safety checks --------\n",
        "    assert all(\n",
        "        h not in unseen_entities and t not in unseen_entities\n",
        "        for h, _, t in train_split\n",
        "    )\n",
        "\n",
        "    assert any(\n",
        "        h in unseen_entities or t in unseen_entities\n",
        "        for h, _, t in valid_split + test_split\n",
        "    )\n",
        "\n",
        "    # -------- output --------\n",
        "    out_dir = OOKB_DIR / dataset_dir.name\n",
        "    out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    write_triples(out_dir / \"train.txt\", train_split)\n",
        "    write_triples(out_dir / \"valid.txt\", valid_split)\n",
        "    write_triples(out_dir / \"test.txt\",  test_split)\n",
        "\n",
        "    # -------- dictionaries --------\n",
        "    entity2id = {e: i for i, e in enumerate(sorted(entities))}\n",
        "    relation2id = {r: i for i, r in enumerate(sorted(relations))}\n",
        "    unseenentity2id = {e: entity2id[e] for e in sorted(unseen_entities)}\n",
        "\n",
        "    with (out_dir / \"entity2id.txt\").open(\"w\") as f:\n",
        "        for e, i in entity2id.items():\n",
        "            f.write(f\"{e}\\t{i}\\n\")\n",
        "\n",
        "    with (out_dir / \"relation2id.txt\").open(\"w\") as f:\n",
        "        for r, i in relation2id.items():\n",
        "            f.write(f\"{r}\\t{i}\\n\")\n",
        "\n",
        "    with (out_dir / \"unseenentity2id.txt\").open(\"w\") as f:\n",
        "        for e, i in unseenentity2id.items():\n",
        "            f.write(f\"{e}\\t{i}\\n\")\n",
        "\n",
        "    print(\n",
        "        f\"  train={len(train_split)} | \"\n",
        "        f\"valid={len(valid_split)} | \"\n",
        "        f\"test={len(test_split)}\"\n",
        "    )\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Run OOKB for all datasets\n",
        "# -------------------------\n",
        "\n",
        "import shutil\n",
        "\n",
        "OOKB_PREDEFINED = {\"WN11\", \"FB13\"}\n",
        "\n",
        "print(\"\\n=== Preparing predefined OOKB datasets (from RAW) ===\")\n",
        "\n",
        "for name in OOKB_PREDEFINED:\n",
        "    src = RAW_DIR / name\n",
        "    dst = OOKB_DIR / name\n",
        "\n",
        "    if not src.exists():\n",
        "        print(f\"[SKIP] {name}: not found in RAW_DIR\")\n",
        "        continue\n",
        "\n",
        "    if dst.exists():\n",
        "        print(f\"[OK] {name}: already exists\")\n",
        "        continue\n",
        "\n",
        "    shutil.copytree(src, dst)\n",
        "    print(f\"[COPIED] {name}\")\n",
        "\n",
        "\n",
        "print(\"\\n=== Generating OOKB splits (custom datasets only) ===\")\n",
        "\n",
        "for dataset_dir in DATA_DIR.iterdir():\n",
        "    if not dataset_dir.is_dir():\n",
        "        continue\n",
        "\n",
        "    if dataset_dir.name in OOKB_PREDEFINED:\n",
        "        continue   # WN11 / FB13 ya están listos\n",
        "\n",
        "    generate_ookb_splits(dataset_dir)\n",
        "\n",
        "print(\"\\n[DONE] All OOKB datasets generated.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnXvTAMhc9Uo",
        "outputId": "0d72e4ec-a34b-480e-8d18-b02a423ea798"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Preparing predefined OOKB datasets (from RAW) ===\n",
            "[COPIED] WN11\n",
            "[COPIED] FB13\n",
            "\n",
            "=== Generating OOKB splits (custom datasets only) ===\n",
            "\n",
            "[OOKB DATASET] CoDEx-M\n",
            "  entities=17050 | unseen=3410\n",
            "  train=133689 | valid=36258 | test=36258\n",
            "\n",
            "[OOKB DATASET] FB15k-237\n",
            "  entities=14541 | unseen=2908\n",
            "  train=199944 | valid=55086 | test=55086\n",
            "\n",
            "[OOKB DATASET] WN18RR\n",
            "  entities=40943 | unseen=8189\n",
            "  train=59476 | valid=16763 | test=16764\n",
            "\n",
            "[DONE] All OOKB datasets generated.\n"
          ]
        }
      ]
    }
  ]
}