{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering."
      ],
      "metadata": {
        "id": "WPj6EBAy0YEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering. Handle datasets in order to make sure test sets do have new relations/entities"
      ],
      "metadata": {
        "id": "-5G5cLK_0awH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ## 1. Environment and GPU sanity check"
      ],
      "metadata": {
        "id": "Mj6g11A-IjnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU check for future pipeline\n",
        "import torch\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA Version:\", torch.version.cuda)\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  print(\"------------No GPU. Set Runtime → Change runtime type → GPU------------\")\n",
        "\n",
        "try:\n",
        "    import torch_geometric\n",
        "    print(\"Torch Geometric:\", torch_geometric.__version__)\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Torch Geometric not found. Installing\")\n",
        "    torch_version = torch.__version__.split(\"+\")[0]\n",
        "    cuda_version = torch.version.cuda.replace(\".\", \"\")\n",
        "\n",
        "    !pip install -q pyg-lib torch-scatter torch-sparse torch-cluster torch-spline-conv \\\n",
        "        -f https://data.pyg.org/whl/torch-{torch_version}+cu{cuda_version}.html\n",
        "\n",
        "    !pip install -q torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eismSo1BBGoH",
        "outputId": "51b9821a-9af0-42c6-ad98-4eaa22dc630d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "CUDA Version: 12.6\n",
            "Torch Geometric: 2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataset download and normalization"
      ],
      "metadata": {
        "id": "7pI_W7KtImWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasets download\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Rutas base\n",
        "PYG_DIR = Path(\"./pyg_temp\")     # Carpeta temporal para descargas de PyG\n",
        "FINAL_DATA_DIR = Path(\"./data\")  # Carpeta de datos principal\n",
        "\n",
        "# Crear carpetas si no existen (idempotente)\n",
        "for directory in [PYG_DIR, FINAL_DATA_DIR]:\n",
        "    directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Estructura de carpetas verificada:\")\n",
        "print(f\"- PYG_DIR: {PYG_DIR.resolve()}\")\n",
        "print(f\"- FINAL_DATA_DIR: {FINAL_DATA_DIR.resolve()}\")\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from torch_geometric.datasets import WordNet18RR, FB15k_237\n",
        "\n",
        "# --- CONFIGURACIÓN ---\n",
        "PYG_DIR = \"./pyg_temp\"  # Carpeta temporal para descargas de PyG\n",
        "FINAL_DATA_DIR = \"./data\" # Tu carpeta de datos principal\n",
        "\n",
        "def standard_to_txt(pyg_dataset, dataset_name):\n",
        "    \"\"\"\n",
        "    Toma los archivos raw descargados por PyG y los mueve a tu carpeta ./data\n",
        "    en formato limpio (head, relation, tail).\n",
        "    \"\"\"\n",
        "    raw_dir = pyg_dataset.raw_dir\n",
        "    target_dir = os.path.join(FINAL_DATA_DIR, dataset_name)\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"\\nProcesando {dataset_name} desde {raw_dir}...\")\n",
        "\n",
        "    # Mapeo de nombres de archivos de PyG a nombres estándar\n",
        "    # PyG a veces usa 'train.txt', a veces otros nombres.\n",
        "    files_map = {\n",
        "        'train': ['train.txt'],\n",
        "        'valid': ['valid.txt', 'valid.csv'],\n",
        "        'test': ['test.txt']\n",
        "    }\n",
        "\n",
        "    for split, possible_names in files_map.items():\n",
        "        found = False\n",
        "        for fname in possible_names:\n",
        "            src_path = os.path.join(raw_dir, fname)\n",
        "            if os.path.exists(src_path):\n",
        "                found = True\n",
        "                dst_path = os.path.join(target_dir, f\"{split}.txt\")\n",
        "\n",
        "                # Leemos con Pandas para asegurarnos de limpiar headers o índices extra\n",
        "                try:\n",
        "                    # FB15k-237 y WN18RR raw suelen venir separados por tabs o espacios\n",
        "                    df = pd.read_csv(src_path, sep=None, engine='python', header=None, on_bad_lines='skip')\n",
        "\n",
        "                    # Quedarnos con las primeras 3 columnas (Head, Rel, Tail)\n",
        "                    # OJO: FB15k-237 a veces viene como (Head, Tail, Rel) o (Head, Rel, Tail)\n",
        "                    # En los raw files de PyG estándar suele ser: Head, Relation, Tail (Strings)\n",
        "                    if df.shape[1] >= 3:\n",
        "                        df = df.iloc[:, :3]\n",
        "\n",
        "                        # Guardamos en formato limpio separado por comas o tabs\n",
        "                        df.to_csv(dst_path, sep=',', index=False, header=False)\n",
        "                        print(f\"  -> {split}.txt guardado en {target_dir} ({len(df)} filas)\")\n",
        "                    else:\n",
        "                        print(f\"  [!] Estructura extraña en {fname}: {df.shape}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  [Error] al procesar {fname}: {e}\")\n",
        "                break\n",
        "\n",
        "        if not found:\n",
        "            print(f\"  [X] No se encontró archivo para split '{split}'\")\n",
        "\n",
        "# --- 1. DESCARGAR WN18RR ---\n",
        "print(\"--- Descargando WordNet18RR usando PyG ---\")\n",
        "# Esto descargará automáticamente los archivos a ./pyg_temp/WordNet18RR/raw\n",
        "dataset_wn = WordNet18RR(root=os.path.join(PYG_DIR, \"WordNet18RR\"))\n",
        "standard_to_txt(dataset_wn, \"WN18RR\")\n",
        "\n",
        "# --- 2. DESCARGAR FB15k-237 ---\n",
        "print(\"--- Descargando FB15k-237 usando PyG ---\")\n",
        "# Esto descargará automáticamente los archivos a ./pyg_temp/FB15k_237/raw\n",
        "dataset_fb = FB15k_237(root=os.path.join(PYG_DIR, \"FB15k-237\"))\n",
        "standard_to_txt(dataset_fb, \"FB15k-237\")\n",
        "\n",
        "print(\"\\n--- ¡LISTO! ---\")\n",
        "print(f\"Ahora tienes WN18RR y FB15k-237 en la carpeta '{FINAL_DATA_DIR}' con el mismo formato que el resto.\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# Set style for plots\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "\n",
        "# --- CONFIGURATION (CORRECTED URLS) ---\n",
        "DATA_DIR = \"./data\"\n",
        "\n",
        "DATASETS = {\n",
        "    \"CoDEx-M\": {\n",
        "        \"base_url\": \"https://raw.githubusercontent.com/tsafavi/codex/master/data/triples/codex-m/\",\n",
        "        \"files\": [\"train.txt\", \"valid.txt\", \"test.txt\"]\n",
        "    },\n",
        "    \"WN11\": {\n",
        "        \"base_url\": \"https://raw.githubusercontent.com/KGCompletion/TransL/master/WN11/\",\n",
        "        \"files\": [\"train.txt\", \"valid.txt\", \"test.txt\"]\n",
        "    },\n",
        "    \"FB13\": {\n",
        "        \"base_url\": \"https://raw.githubusercontent.com/KGCompletion/TransL/master/FB13/\",\n",
        "        \"files\": [\"train.txt\", \"valid.txt\", \"test.txt\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# --- HELPER FUNCTIONS ---\n",
        "def download_file(url, save_path):\n",
        "    \"\"\"Downloads a file from a URL if it doesn't exist.\"\"\"\n",
        "    if not os.path.exists(save_path):\n",
        "        print(f\"Downloading {save_path}...\")\n",
        "        try:\n",
        "            r = requests.get(url)\n",
        "            r.raise_for_status()\n",
        "            with open(save_path, 'wb') as f:\n",
        "                f.write(r.content)\n",
        "            print(f\"  -> Success!\")\n",
        "        except Exception as e:\n",
        "            print(f\"  -> Failed to download {url}: {e}\")\n",
        "    else:\n",
        "        print(f\"Found local: {save_path}\")\n",
        "\n",
        "def load_dataset(name, config):\n",
        "    \"\"\"Loads train/valid/test into a dictionary of DataFrames.\"\"\"\n",
        "    path = os.path.join(DATA_DIR, name)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "    dfs = {}\n",
        "    for file_name in config[\"files\"]:\n",
        "        # 1. Download\n",
        "        url = config[\"base_url\"] + file_name\n",
        "        local_path = os.path.join(path, file_name)\n",
        "        download_file(url, local_path)\n",
        "\n",
        "        # 2. Load to Pandas\n",
        "        # Try-catch to handle potential parsing issues\n",
        "        try:\n",
        "            # We use engine='python' and sep=None to auto-detect tab or space separators\n",
        "            # This makes it robust against different format standards\n",
        "            df = pd.read_csv(local_path, sep=None, engine='python',\n",
        "                             names=['head', 'relation', 'tail'],\n",
        "                             on_bad_lines='skip')\n",
        "\n",
        "            # Clean up: Sometimes the last column has a 4th value (label 1/-1) in older datasets\n",
        "            # For EDA we usually just want the triplet. Let's check shape.\n",
        "            if df.shape[1] > 3:\n",
        "                df = df.iloc[:, :3]\n",
        "                df.columns = ['head', 'relation', 'tail']\n",
        "\n",
        "            dfs[file_name.replace('.txt', '')] = df\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {file_name}: {e}\")\n",
        "\n",
        "    return dfs\n",
        "\n",
        "# --- EXECUTION ---\n",
        "print(\"--- STARTING DATA LOADING ---\")\n",
        "kg_data = {}\n",
        "\n",
        "for ds_name, config in DATASETS.items():\n",
        "    print(f\"\\nProcessing {ds_name}...\")\n",
        "    kg_data[ds_name] = load_dataset(ds_name, config)\n",
        "\n",
        "    # Quick Sanity Check\n",
        "    if 'train' in kg_data[ds_name]:\n",
        "        count = len(kg_data[ds_name]['train'])\n",
        "        print(f\"  -> Loaded {ds_name} Train: {count} triplets\")\n",
        "        print(f\"  -> Sample: {kg_data[ds_name]['train'].iloc[0].values}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4frQnX-Buym",
        "outputId": "4e6ed9e3-f38e-427b-f901-6e8c96b5f658"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estructura de carpetas verificada:\n",
            "- PYG_DIR: /content/pyg_temp\n",
            "- FINAL_DATA_DIR: /content/data\n",
            "--- Descargando WordNet18RR usando PyG ---\n",
            "\n",
            "Procesando WN18RR desde pyg_temp/WordNet18RR/raw...\n",
            "  -> train.txt guardado en ./data/WN18RR (86835 filas)\n",
            "  -> valid.txt guardado en ./data/WN18RR (3034 filas)\n",
            "  -> test.txt guardado en ./data/WN18RR (3134 filas)\n",
            "--- Descargando FB15k-237 usando PyG ---\n",
            "\n",
            "Procesando FB15k-237 desde pyg_temp/FB15k-237/raw...\n",
            "  -> train.txt guardado en ./data/FB15k-237 (272115 filas)\n",
            "  -> valid.txt guardado en ./data/FB15k-237 (17535 filas)\n",
            "  -> test.txt guardado en ./data/FB15k-237 (20466 filas)\n",
            "\n",
            "--- ¡LISTO! ---\n",
            "Ahora tienes WN18RR y FB15k-237 en la carpeta './data' con el mismo formato que el resto.\n",
            "--- STARTING DATA LOADING ---\n",
            "\n",
            "Processing CoDEx-M...\n",
            "Found local: ./data/CoDEx-M/train.txt\n",
            "Found local: ./data/CoDEx-M/valid.txt\n",
            "Found local: ./data/CoDEx-M/test.txt\n",
            "  -> Loaded CoDEx-M Train: 185584 triplets\n",
            "  -> Sample: ['Q108946' 'P161' 'Q39792']\n",
            "\n",
            "Processing WN11...\n",
            "Found local: ./data/WN11/train.txt\n",
            "Found local: ./data/WN11/valid.txt\n",
            "Found local: ./data/WN11/test.txt\n",
            "  -> Loaded WN11 Train: 112581 triplets\n",
            "  -> Sample: ['__spiritual_bouquet_1' '_type_of' '__sympathy_card_1']\n",
            "\n",
            "Processing FB13...\n",
            "Found local: ./data/FB13/train.txt\n",
            "Found local: ./data/FB13/valid.txt\n",
            "Found local: ./data/FB13/test.txt\n",
            "  -> Loaded FB13 Train: 316232 triplets\n",
            "  -> Sample: ['antoine_brutus_menier' 'religion' 'roman_catholic_church']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Inductive relation-based splits (InGram setup)"
      ],
      "metadata": {
        "id": "zjt1eS4UIpv0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "3PKZGpjj0VUN",
        "outputId": "b03eb6dc-60ea-481b-a214-1bfc707d0145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] Loading base dataset: WN18RR\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2548228391.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2548228391.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n[INFO] Loading base dataset: {DATASET_NAME}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mtrain_triples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_triples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mvalid_triples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_triples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mtest_triples\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mread_triples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2548228391.py\u001b[0m in \u001b[0;36mread_triples\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mtriples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtriples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
          ]
        }
      ],
      "source": [
        "# Inductive relation-based partitioning for Knowledge Graph datasets\n",
        "\n",
        "\"\"\"\n",
        "Inductive relation-based partitioning for Knowledge Graph datasets.\n",
        "Generates NL-25 / NL-50 / NL-75 / NL-100 splits where relations are unseen in train.\n",
        "\n",
        "Assumptions:\n",
        "- Input datasets follow:\n",
        "  data/DATASET/train.txt\n",
        "  data/DATASET/valid.txt\n",
        "  data/DATASET/test.txt\n",
        "- Triplets are tab-separated: h \\t r \\t t\n",
        "- Entities may appear in any split (allowed)\n",
        "- Relations selected as \"new\" do NOT appear in train\n",
        "- valid and test contain ONLY new relations\n",
        "- Reproducible via fixed seed\n",
        "- No external deps beyond Python stdlib (+ optional numpy, not required)\n",
        "\n",
        "Author: you + ChatGPT\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "BASE_DATA_DIR = \"data\"\n",
        "DATASET_NAME = \"WN18RR\"      # change if needed\n",
        "SEED = 42\n",
        "\n",
        "ALPHAS = {\n",
        "    \"NL-25\": 0.25,\n",
        "    \"NL-50\": 0.50,\n",
        "    \"NL-75\": 0.75,\n",
        "    \"NL-100\": 1.00,\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# UTILS\n",
        "# =========================\n",
        "def read_triples(path):\n",
        "    triples = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            h, r, t = line.split(\"\\t\")\n",
        "            triples.append((h, r, t))\n",
        "    return triples\n",
        "\n",
        "\n",
        "def write_triples(path, triples):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for h, r, t in triples:\n",
        "            f.write(f\"{h}\\t{r}\\t{t}\\n\")\n",
        "\n",
        "\n",
        "def ensure_dir(path):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# MAIN LOGIC\n",
        "# =========================\n",
        "def main():\n",
        "    random.seed(SEED)\n",
        "\n",
        "    dataset_dir = os.path.join(BASE_DATA_DIR, DATASET_NAME)\n",
        "    train_path = os.path.join(dataset_dir, \"train.txt\")\n",
        "    valid_path = os.path.join(dataset_dir, \"valid.txt\")\n",
        "    test_path  = os.path.join(dataset_dir, \"test.txt\")\n",
        "\n",
        "    print(f\"\\n[INFO] Loading base dataset: {DATASET_NAME}\")\n",
        "\n",
        "    train_triples = read_triples(train_path)\n",
        "    valid_triples = read_triples(valid_path)\n",
        "    test_triples  = read_triples(test_path)\n",
        "\n",
        "    all_triples = train_triples + valid_triples + test_triples\n",
        "\n",
        "    # Group triples by relation\n",
        "    rel2triples = defaultdict(list)\n",
        "    for h, r, t in all_triples:\n",
        "        rel2triples[r].append((h, r, t))\n",
        "\n",
        "    all_relations = sorted(rel2triples.keys())\n",
        "    num_relations = len(all_relations)\n",
        "\n",
        "    print(f\"[STATS] Total triples      : {len(all_triples)}\")\n",
        "    print(f\"[STATS] Total relations    : {num_relations}\")\n",
        "\n",
        "    for split_name, alpha in ALPHAS.items():\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(f\"[SPLIT] Generating {split_name} (alpha={alpha})\")\n",
        "\n",
        "        num_new_rel = int(round(num_relations * alpha))\n",
        "\n",
        "        if num_new_rel > num_relations:\n",
        "            num_new_rel = num_relations\n",
        "\n",
        "        shuffled_relations = all_relations[:]\n",
        "        random.shuffle(shuffled_relations)\n",
        "\n",
        "        new_relations = set(shuffled_relations[:num_new_rel])\n",
        "        old_relations = set(shuffled_relations[num_new_rel:])\n",
        "\n",
        "        # Build splits\n",
        "        train_split = []\n",
        "        valid_split = []\n",
        "        test_split  = []\n",
        "\n",
        "        for r in old_relations:\n",
        "            train_split.extend(rel2triples[r])\n",
        "\n",
        "        new_relation_triples = []\n",
        "        for r in new_relations:\n",
        "            new_relation_triples.extend(rel2triples[r])\n",
        "\n",
        "        # Split new-relation triples into valid/test (50/50)\n",
        "        random.shuffle(new_relation_triples)\n",
        "        mid = len(new_relation_triples) // 2\n",
        "        valid_split = new_relation_triples[:mid]\n",
        "        test_split  = new_relation_triples[mid:]\n",
        "\n",
        "        # Safety checks\n",
        "        train_rels = {r for _, r, _ in train_split}\n",
        "        valid_rels = {r for _, r, _ in valid_split}\n",
        "        test_rels  = {r for _, r, _ in test_split}\n",
        "\n",
        "        assert train_rels.isdisjoint(new_relations), \"Leakage: new relations in train!\"\n",
        "        assert valid_rels.issubset(new_relations), \"Invalid relation in valid!\"\n",
        "        assert test_rels.issubset(new_relations), \"Invalid relation in test!\"\n",
        "\n",
        "        # Output directory\n",
        "        out_dir = os.path.join(dataset_dir, split_name)\n",
        "        ensure_dir(out_dir)\n",
        "\n",
        "        write_triples(os.path.join(out_dir, \"train.txt\"), train_split)\n",
        "        write_triples(os.path.join(out_dir, \"valid.txt\"), valid_split)\n",
        "        write_triples(os.path.join(out_dir, \"test.txt\"),  test_split)\n",
        "\n",
        "        # Report\n",
        "        print(f\"[STATS] #new relations     : {len(new_relations)}\")\n",
        "        print(f\"[STATS] train triples     : {len(train_split)}\")\n",
        "        print(f\"[STATS] valid triples     : {len(valid_split)}\")\n",
        "        print(f\"[STATS] test  triples     : {len(test_split)}\")\n",
        "        print(f\"[PATH ] Written to        : {out_dir}\")\n",
        "\n",
        "    print(\"\\n[DONE] All NL-* splits generated successfully.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LzACn0anEdTe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}