{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "746008f4",
      "metadata": {
        "id": "746008f4"
      },
      "source": [
        "# **Maestr√≠a en Inteligencia Artificial Aplicada**\n",
        "\n",
        "## **Curso: Proyecto Integrador**\n",
        "\n",
        "### Tecnol√≥gico de Monterrey\n",
        "\n",
        "\n",
        "## **Actividad de la Semana 02**\n",
        "### **EDA de los conjuntos de datos seleccionados**\n",
        "\n",
        "\n",
        "Equipo 32\n",
        "\n",
        "Jos√© Adan Vega P√©rez [A01796093]\n",
        "\n",
        "Silvia Xochitl Iba√±ez Vara [A01795200]\n",
        "\n",
        "Diego Andr√©s Bernal D√≠az [A01795975]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requisitos"
      ],
      "metadata": {
        "id": "CNy-8gEVJju-"
      },
      "id": "CNy-8gEVJju-"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
        "print(\"Versi√≥n CUDA:\", torch.version.cuda)\n",
        "\n",
        "assert torch.cuda.is_available(), (\"No GPU. Set Runtime ‚Üí Change runtime type ‚Üí GPU\")"
      ],
      "metadata": {
        "id": "DPVNYijy_p6W"
      },
      "id": "DPVNYijy_p6W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "try:\n",
        "    import torch_geometric\n",
        "except ModuleNotFoundError:\n",
        "    torch_version = torch.__version__.split(\"+\")[0]\n",
        "    cuda_version = torch.version.cuda.replace(\".\", \"\")\n",
        "\n",
        "    !pip install -q pyg-lib torch-scatter torch-sparse torch-cluster torch-spline-conv \\\n",
        "        -f https://data.pyg.org/whl/torch-{torch_version}+cu{cuda_version}.html\n",
        "\n",
        "    !pip install -q torch-geometric\n"
      ],
      "metadata": {
        "id": "7OzG8er9c8a_"
      },
      "id": "7OzG8er9c8a_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_geometric\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Torch Geometric:\", torch_geometric.__version__)\n"
      ],
      "metadata": {
        "id": "yioR74RrcWqV"
      },
      "id": "yioR74RrcWqV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Rutas base\n",
        "PYG_DIR = Path(\"./pyg_temp\")     # Carpeta temporal para descargas de PyG\n",
        "FINAL_DATA_DIR = Path(\"./data\")  # Carpeta de datos principal\n",
        "\n",
        "# Crear carpetas si no existen (idempotente)\n",
        "for directory in [PYG_DIR, FINAL_DATA_DIR]:\n",
        "    directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Estructura de carpetas verificada:\")\n",
        "print(f\"- PYG_DIR: {PYG_DIR.resolve()}\")\n",
        "print(f\"- FINAL_DATA_DIR: {FINAL_DATA_DIR.resolve()}\")\n"
      ],
      "metadata": {
        "id": "jMlIdhmHE3_m"
      },
      "id": "jMlIdhmHE3_m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5573ff36",
      "metadata": {
        "id": "5573ff36"
      },
      "source": [
        "## Obtencion de los datasets Wordnet18 y Freebase 15k de pytorch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c39fe38",
      "metadata": {
        "id": "2c39fe38"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from torch_geometric.datasets import WordNet18RR, FB15k_237\n",
        "\n",
        "# --- CONFIGURACI√ìN ---\n",
        "PYG_DIR = \"./pyg_temp\"  # Carpeta temporal para descargas de PyG\n",
        "FINAL_DATA_DIR = \"./data\" # Tu carpeta de datos principal\n",
        "\n",
        "def standard_to_txt(pyg_dataset, dataset_name):\n",
        "    \"\"\"\n",
        "    Toma los archivos raw descargados por PyG y los mueve a tu carpeta ./data\n",
        "    en formato limpio (head, relation, tail).\n",
        "    \"\"\"\n",
        "    raw_dir = pyg_dataset.raw_dir\n",
        "    target_dir = os.path.join(FINAL_DATA_DIR, dataset_name)\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"\\nProcesando {dataset_name} desde {raw_dir}...\")\n",
        "\n",
        "    # Mapeo de nombres de archivos de PyG a nombres est√°ndar\n",
        "    # PyG a veces usa 'train.txt', a veces otros nombres.\n",
        "    files_map = {\n",
        "        'train': ['train.txt'],\n",
        "        'valid': ['valid.txt', 'valid.csv'],\n",
        "        'test': ['test.txt']\n",
        "    }\n",
        "\n",
        "    for split, possible_names in files_map.items():\n",
        "        found = False\n",
        "        for fname in possible_names:\n",
        "            src_path = os.path.join(raw_dir, fname)\n",
        "            if os.path.exists(src_path):\n",
        "                found = True\n",
        "                dst_path = os.path.join(target_dir, f\"{split}.txt\")\n",
        "\n",
        "                # Leemos con Pandas para asegurarnos de limpiar headers o √≠ndices extra\n",
        "                try:\n",
        "                    # FB15k-237 y WN18RR raw suelen venir separados por tabs o espacios\n",
        "                    df = pd.read_csv(src_path, sep=None, engine='python', header=None, on_bad_lines='skip')\n",
        "\n",
        "                    # Quedarnos con las primeras 3 columnas (Head, Rel, Tail)\n",
        "                    # OJO: FB15k-237 a veces viene como (Head, Tail, Rel) o (Head, Rel, Tail)\n",
        "                    # En los raw files de PyG est√°ndar suele ser: Head, Relation, Tail (Strings)\n",
        "                    if df.shape[1] >= 3:\n",
        "                        df = df.iloc[:, :3]\n",
        "\n",
        "                        # Guardamos en formato limpio separado por comas o tabs\n",
        "                        df.to_csv(dst_path, sep=',', index=False, header=False)\n",
        "                        print(f\"  -> {split}.txt guardado en {target_dir} ({len(df)} filas)\")\n",
        "                    else:\n",
        "                        print(f\"  [!] Estructura extra√±a en {fname}: {df.shape}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  [Error] al procesar {fname}: {e}\")\n",
        "                break\n",
        "\n",
        "        if not found:\n",
        "            print(f\"  [X] No se encontr√≥ archivo para split '{split}'\")\n",
        "\n",
        "# --- 1. DESCARGAR WN18RR ---\n",
        "print(\"--- Descargando WordNet18RR usando PyG ---\")\n",
        "# Esto descargar√° autom√°ticamente los archivos a ./pyg_temp/WordNet18RR/raw\n",
        "dataset_wn = WordNet18RR(root=os.path.join(PYG_DIR, \"WordNet18RR\"))\n",
        "standard_to_txt(dataset_wn, \"WN18RR\")\n",
        "\n",
        "# --- 2. DESCARGAR FB15k-237 ---\n",
        "print(\"--- Descargando FB15k-237 usando PyG ---\")\n",
        "# Esto descargar√° autom√°ticamente los archivos a ./pyg_temp/FB15k_237/raw\n",
        "dataset_fb = FB15k_237(root=os.path.join(PYG_DIR, \"FB15k-237\"))\n",
        "standard_to_txt(dataset_fb, \"FB15k-237\")\n",
        "\n",
        "print(\"\\n--- ¬°LISTO! ---\")\n",
        "print(f\"Ahora tienes WN18RR y FB15k-237 en la carpeta '{FINAL_DATA_DIR}' con el mismo formato que el resto.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b809ee6d",
      "metadata": {
        "id": "b809ee6d"
      },
      "source": [
        "## Obtencion de CoDEx-M, WordNet11, FreeBase13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07094be0",
      "metadata": {
        "id": "07094be0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# Set style for plots\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "\n",
        "# --- CONFIGURATION (CORRECTED URLS) ---\n",
        "DATA_DIR = \"./data\"\n",
        "\n",
        "DATASETS = {\n",
        "    \"CoDEx-M\": {\n",
        "        \"base_url\": \"https://raw.githubusercontent.com/tsafavi/codex/master/data/triples/codex-m/\",\n",
        "        \"files\": [\"train.txt\", \"valid.txt\", \"test.txt\"]\n",
        "    },\n",
        "    \"WN11\": {\n",
        "        \"base_url\": \"https://raw.githubusercontent.com/KGCompletion/TransL/master/WN11/\",\n",
        "        \"files\": [\"train.txt\", \"valid.txt\", \"test.txt\"]\n",
        "    },\n",
        "    \"FB13\": {\n",
        "        \"base_url\": \"https://raw.githubusercontent.com/KGCompletion/TransL/master/FB13/\",\n",
        "        \"files\": [\"train.txt\", \"valid.txt\", \"test.txt\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# --- HELPER FUNCTIONS ---\n",
        "def download_file(url, save_path):\n",
        "    \"\"\"Downloads a file from a URL if it doesn't exist.\"\"\"\n",
        "    if not os.path.exists(save_path):\n",
        "        print(f\"Downloading {save_path}...\")\n",
        "        try:\n",
        "            r = requests.get(url)\n",
        "            r.raise_for_status()\n",
        "            with open(save_path, 'wb') as f:\n",
        "                f.write(r.content)\n",
        "            print(f\"  -> Success!\")\n",
        "        except Exception as e:\n",
        "            print(f\"  -> Failed to download {url}: {e}\")\n",
        "    else:\n",
        "        print(f\"Found local: {save_path}\")\n",
        "\n",
        "def load_dataset(name, config):\n",
        "    \"\"\"Loads train/valid/test into a dictionary of DataFrames.\"\"\"\n",
        "    path = os.path.join(DATA_DIR, name)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "    dfs = {}\n",
        "    for file_name in config[\"files\"]:\n",
        "        # 1. Download\n",
        "        url = config[\"base_url\"] + file_name\n",
        "        local_path = os.path.join(path, file_name)\n",
        "        download_file(url, local_path)\n",
        "\n",
        "        # 2. Load to Pandas\n",
        "        # Try-catch to handle potential parsing issues\n",
        "        try:\n",
        "            # We use engine='python' and sep=None to auto-detect tab or space separators\n",
        "            # This makes it robust against different format standards\n",
        "            df = pd.read_csv(local_path, sep=None, engine='python',\n",
        "                             names=['head', 'relation', 'tail'],\n",
        "                             on_bad_lines='skip')\n",
        "\n",
        "            # Clean up: Sometimes the last column has a 4th value (label 1/-1) in older datasets\n",
        "            # For EDA we usually just want the triplet. Let's check shape.\n",
        "            if df.shape[1] > 3:\n",
        "                df = df.iloc[:, :3]\n",
        "                df.columns = ['head', 'relation', 'tail']\n",
        "\n",
        "            dfs[file_name.replace('.txt', '')] = df\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {file_name}: {e}\")\n",
        "\n",
        "    return dfs\n",
        "\n",
        "# --- EXECUTION ---\n",
        "print(\"--- STARTING DATA LOADING ---\")\n",
        "kg_data = {}\n",
        "\n",
        "for ds_name, config in DATASETS.items():\n",
        "    print(f\"\\nProcessing {ds_name}...\")\n",
        "    kg_data[ds_name] = load_dataset(ds_name, config)\n",
        "\n",
        "    # Quick Sanity Check\n",
        "    if 'train' in kg_data[ds_name]:\n",
        "        count = len(kg_data[ds_name]['train'])\n",
        "        print(f\"  -> Loaded {ds_name} Train: {count} triplets\")\n",
        "        print(f\"  -> Sample: {kg_data[ds_name]['train'].iloc[0].values}\")\n",
        "\n",
        "print(\"\\n--- READY FOR EDA ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cda3bcb2",
      "metadata": {
        "id": "cda3bcb2"
      },
      "source": [
        "# 1. Macro-Estadisticas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18b457e0",
      "metadata": {
        "id": "18b457e0"
      },
      "source": [
        "Before diving deep, you need a high-level comparison table. This establishes the scale.\n",
        "\n",
        "    Metrics:\n",
        "        Total Entities\n",
        "        Total Relations\n",
        "        Total Triplets\n",
        "        Split distribution: Count of Train / Validation / Test triples.\n",
        "        Graph Density: (Is the graph sparse or dense?).\n",
        "        Average Degree: On average, how many connections does a node have?\n",
        "\n",
        "    Why it matters: Sparse graphs (like WordNet) are harder for GNNs (Graph Neural Networks) because there is less \"message passing.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0810c012",
      "metadata": {
        "id": "0810c012"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "def calculate_macro_stats(base_dir=FINAL_DATA_DIR):\n",
        "    stats_list = []\n",
        "\n",
        "    # Get all subdirectories in ./data\n",
        "    dataset_names = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
        "\n",
        "    print(f\"Found datasets: {dataset_names}\")\n",
        "\n",
        "    for name in dataset_names:\n",
        "        ds_path = os.path.join(base_dir, name)\n",
        "\n",
        "        # Initialize sets to track unique entities and relations across ALL splits\n",
        "        all_entities = set()\n",
        "        all_relations = set()\n",
        "\n",
        "        split_counts = {'train': 0, 'valid': 0, 'test': 0}\n",
        "        total_triples = 0\n",
        "\n",
        "        # Process each split\n",
        "        for split in ['train', 'valid', 'test']:\n",
        "            file_path = os.path.join(ds_path, f\"{split}.txt\")\n",
        "\n",
        "            if os.path.exists(file_path):\n",
        "                try:\n",
        "                    # Robust loading: handles comma, tab, or space separators automatically\n",
        "                    df = pd.read_csv(file_path, sep=None, engine='python', header=None, on_bad_lines='skip')\n",
        "\n",
        "                    # Ensure we have at least 3 columns\n",
        "                    if df.shape[1] >= 3:\n",
        "                        # Extract Head, Relation, Tail (assuming 0=Head, 1=Rel, 2=Tail based on standard formats)\n",
        "                        # NOTE: Some datasets are H, T, R. Others H, R, T.\n",
        "                        # For simply COUNTING entities/relations, column order doesn't matter much\n",
        "                        # as long as we grab the entities and relation columns.\n",
        "                        # We assume standard H, R, T or H, T, R. We'll take col 1 as relation mostly.\n",
        "                        # But to be safe for counts, we treat the \"middle\" as relation usually,\n",
        "                        # or we can check unique counts to guess, but let's stick to standard H, R, T logic here.\n",
        "\n",
        "                        heads = df.iloc[:, 0].astype(str)\n",
        "                        rels = df.iloc[:, 1].astype(str)\n",
        "                        tails = df.iloc[:, 2].astype(str)\n",
        "\n",
        "                        # Update counts\n",
        "                        count = len(df)\n",
        "                        split_counts[split] = count\n",
        "                        total_triples += count\n",
        "\n",
        "                        # Update Unique Sets\n",
        "                        all_entities.update(heads.unique())\n",
        "                        all_entities.update(tails.unique())\n",
        "                        all_relations.update(rels.unique())\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {name}/{split}: {e}\")\n",
        "\n",
        "        # Calculate Derived Metrics\n",
        "        num_entities = len(all_entities)\n",
        "        num_relations = len(all_relations)\n",
        "\n",
        "        # Avoid division by zero\n",
        "        if num_entities > 0:\n",
        "            # Avg Degree = Triples / Entities\n",
        "            avg_degree = total_triples / num_entities\n",
        "\n",
        "            # Density = Triples / (Entities * (Entities - 1))  [Directed Graph]\n",
        "            density = total_triples / (num_entities * (num_entities - 1))\n",
        "        else:\n",
        "            avg_degree = 0\n",
        "            density = 0\n",
        "\n",
        "        stats_list.append({\n",
        "            'Dataset': name,\n",
        "            'Entities (|E|)': num_entities,\n",
        "            'Relations (|R|)': num_relations,\n",
        "            'Triples (Total)': total_triples,\n",
        "            'Train': split_counts['train'],\n",
        "            'Valid': split_counts['valid'],\n",
        "            'Test': split_counts['test'],\n",
        "            'Avg Degree': round(avg_degree, 4),\n",
        "            'Graph Density': density # Keep distinct to see scientific notation if needed\n",
        "        })\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_stats = pd.DataFrame(stats_list)\n",
        "\n",
        "    # Set Dataset as index\n",
        "    if not df_stats.empty:\n",
        "        df_stats = df_stats.set_index('Dataset')\n",
        "\n",
        "    return df_stats\n",
        "\n",
        "# --- EXECUTE ---\n",
        "df_macro_stats = calculate_macro_stats()\n",
        "\n",
        "# Display formatted table\n",
        "# We use style to format the Density column because it is usually very small (e.g., 0.0001)\n",
        "display(df_macro_stats.style.format({\n",
        "    'Graph Density': '{:.6f}',\n",
        "    'Avg Degree': '{:.2f}'\n",
        "}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc1ead0b",
      "metadata": {
        "id": "fc1ead0b"
      },
      "source": [
        "# 2. An√°lisis de relaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ec45c66",
      "metadata": {
        "id": "9ec45c66"
      },
      "source": [
        "Knowledge Graphs rarely have balanced relations. Some relations are incredibly common (e.g., gender), others are rare.\n",
        "\n",
        "    Analysis:\n",
        "\n",
        "        Frequency Histogram: Plot the count of triplets per relation.\n",
        "\n",
        "        Head vs. Tail Relations: Identify which relations make up the top 50% of the data.\n",
        "\n",
        "    Relevance to Project: If you are extrapolating, it is easier to predict a common relation for a new node than a rare one. If CoDEx-M is more balanced than FB13, your model might perform more consistently there."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d32edbb",
      "metadata": {
        "id": "1d32edbb"
      },
      "source": [
        "# 3. Node Degree Distribution (The \"Hubs\" Analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e4e07e1",
      "metadata": {
        "id": "8e4e07e1"
      },
      "source": [
        "This is crucial for graph embeddings. Real-world graphs follow a Power Law (Long Tail).\n",
        "\n",
        "    Analysis:\n",
        "\n",
        "        Plot the distribution of node degrees (log-log scale).\n",
        "\n",
        "        Identify \"Hubs\" (super-connected nodes, like \"USA\" in Freebase) vs. \"Tail Entities\" (nodes with 1 or 2 connections).\n",
        "\n",
        "    Relevance to Extrapolation (CRITICAL):\n",
        "\n",
        "        Hypothesis: New entities (unseen) that connect to Hubs are easier to embed because the Hub has a stable, rich representation. New entities that connect only to other sparse nodes are extremely hard to predict.\n",
        "\n",
        "        Action: Check what percentage of entities in WN11/FB13 have a degree <3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e336003",
      "metadata": {
        "id": "0e336003"
      },
      "source": [
        "# 4. Connectivity Patterns (The \"Structure\" Analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdfd3b22",
      "metadata": {
        "id": "bdfd3b22"
      },
      "source": [
        "Since WN11 comes from WordNet (hierarchy) and FB13 from Freebase (facts), their shapes are different.\n",
        "\n",
        "    Analysis:\n",
        "\n",
        "        Clustering Coefficient: Do neighbors of a node know each other? (Triangles).\n",
        "\n",
        "        Diameter / Average Path Length: How many hops to get from node A to node B?\n",
        "\n",
        "    Relevance:\n",
        "\n",
        "        WN11: Likely has low clustering (tree structure, hierarchies don't form many triangles).\n",
        "\n",
        "        FB13/CoDEx: Likely higher clustering (social networks, movies). GNNs behave differently on trees vs. clustered graphs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f75828eb",
      "metadata": {
        "id": "f75828eb"
      },
      "source": [
        "# 5. The \"Inductive Bias\" Analysis (The Most Important for You)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fa95e30",
      "metadata": {
        "id": "9fa95e30"
      },
      "source": [
        "Since your project is about unseen entities, you must analyze the \"gap\" between Train and Test. Standard EDAs don't do this, but you must.\n",
        "\n",
        "A. Unseen Entity Ratio (if using standard splits):\n",
        "\n",
        "    In the Test set, how many entities are strictly new?\n",
        "\n",
        "    (Note: If you use the specific benchmarks from Hamaguchi et al. or GraIL, this is 100%, but you should verify it).\n",
        "\n",
        "B. \"Connectivity to Known\" Ratio:\n",
        "\n",
        "    For the triples in the Test set (involving new entities), who are they connected to?\n",
        "\n",
        "        Scenario A: New Entity\n",
        "\n",
        "        Old Entity (Seen in Train).\n",
        "\n",
        "        Scenario B: New Entity\n",
        "\n",
        "\n",
        "        New Entity.\n",
        "\n",
        "    Why it matters:\n",
        "\n",
        "        Scenario A is \"Transductive-Inductive\". Easier. The model uses the Old Entity as an anchor.\n",
        "\n",
        "        Scenario B is \"Fully Inductive\". Extremely hard. If a new node only connects to another new node, the model is \"blind.\"\n",
        "\n",
        "    The Chart: A bar chart showing the % of test triples that connect to Seen vs. Unseen nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e7ed25f",
      "metadata": {
        "id": "6e7ed25f"
      },
      "source": [
        "# 6. Text/Semantics Analysis (Optional but recommended)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dbbd82f",
      "metadata": {
        "id": "8dbbd82f"
      },
      "source": [
        "Since you mentioned using \"descriptions\" or \"external info\" for extrapolation.\n",
        "\n",
        "    Analysis: Calculate the average length of text descriptions for entities in CoDEx vs FB13.\n",
        "\n",
        "    Relevance: If FB13 has rich text and WN11 has short definitions, your text-based extrapolation model might work better on FB13."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estructura de los datos"
      ],
      "metadata": {
        "id": "0GM7RDPfjj9u"
      },
      "id": "0GM7RDPfjj9u"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found datasets: ['WN18RR', 'FB15k-237', 'FB13', 'CoDEx-M', 'WN11']\n"
      ],
      "metadata": {
        "id": "8McHmcnFR3w-"
      },
      "id": "8McHmcnFR3w-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Dataset   | Entities (&#124;E&#124;) | Relations (&#124;R&#124;) | Triples (Total) | Train   | Valid | Test   | Avg Degree | Graph Density |\n",
        "|-----------|--------------------------|---------------------------|-----------------|---------|-------|--------|------------|---------------|\n",
        "| WN18RR    | 40,943                   | 11                        | 93,003          | 86,835  | 3,034 | 3,134  | 2.27       | 0.000055      |\n",
        "| FB15k-237 | 14,541                   | 237                       | 310,116         | 272,115 | 17,535| 20,466 | 21.33      | 0.001467      |\n",
        "| FB13      | 75,043                   | 13                        | 375,514         | 316,232 | 11,816| 47,466 | 5.00       | 0.000067      |\n",
        "| CoDEx-M   | 17,050                   | 51                        | 206,205         | 185,584 | 10,310| 10,311 | 12.09      | 0.000709      |\n",
        "| WN11      | 38,588                   | 11                        | 138,887         | 112,581 | 5,218 | 21,088 | 3.60       | 0.000093      |\n"
      ],
      "metadata": {
        "id": "RDX9899WR5-Z"
      },
      "id": "RDX9899WR5-Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Descripci√≥n del dataset\n",
        "   - entidades\n",
        "   - relaciones\n",
        "   - tripletas\n",
        "   - cardinalidad por split\n",
        "   - tipo de dato (categ√≥rico, simb√≥lico)\n",
        "\n",
        "2. Estad√≠sticas estructurales\n",
        "   - grado promedio\n",
        "   - densidad\n",
        "   - tripletas por entidad\n",
        "   - tripletas por relaci√≥n\n",
        "\n",
        "3. Frecuencias de relaciones\n",
        "   - tabla\n",
        "   - top 10 + cola larga\n",
        "\n",
        "4. Valores faltantes\n",
        "   - NO aplica: En los grafos de conocimiento no existen valores faltantes en el sentido tabular cl√°sico, ya que cada registro corresponde a una relaci√≥n completa entre dos entidades. La ‚Äúinformaci√≥n incompleta‚Äù se manifiesta √∫nicamente de forma estructural mediante nodos con baja conectividad, lo cual se analiza posteriormente en la secci√≥n de conectividad del grafo\n"
      ],
      "metadata": {
        "id": "ryiXNFOmjvaE"
      },
      "id": "ryiXNFOmjvaE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An√°lisis univariante"
      ],
      "metadata": {
        "id": "kbG6QWoljysM"
      },
      "id": "kbG6QWoljysM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que los datos corresponden a grafos de conocimiento representados mediante tripletas\n",
        "(\n",
        "‚Ñé\n",
        ",\n",
        "ùëü\n",
        ",\n",
        "ùë°\n",
        ")\n",
        "(h,r,t), las variables originales son categ√≥ricas y no contienen atributos num√©ricos continuos. Por ello, el an√°lisis univariante se realiza sobre m√©tricas estructurales derivadas del grafo, en lugar de sobre los identificadores de entidades o relaciones.\n",
        "\n",
        "En esta etapa se analizan la distribuci√≥n del grado de las entidades, la frecuencia de aparici√≥n de entidades en las tripletas y la frecuencia de los distintos tipos de relaci√≥n. Asimismo, se identifican las entidades con mayor grado (hubs), ya que concentran una parte significativa de las conexiones del grafo y pueden influir en el comportamiento de los modelos de aprendizaje. Las visualizaciones empleadas consisten en histogramas y gr√°ficos de barras, adecuados para m√©tricas discretas y distribuciones con cola larga. No se utilizan boxplots, ya que no existen variables num√©ricas continuas y este tipo de representaci√≥n no aporta informaci√≥n relevante en este contexto"
      ],
      "metadata": {
        "id": "U6iMC0xzNtwC"
      },
      "id": "U6iMC0xzNtwC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "El proyecto.\n",
        "1. Histograma de grados de entidades\n",
        "2. Gr√°fico de barras de frecuencia de relaciones\n",
        "3. Distribuci√≥n del n√∫mero de tripletas por entidad\n",
        "4. Top-N hubs (barplot)\n",
        "\n",
        "Boxplots no aplican (no hay datos continuos):\n",
        "\n",
        "Dado que las variables del grafo son categ√≥ricas y no contienen atributos num√©ricos continuos, no se emplean boxplots. En su lugar, se utilizan histogramas y gr√°ficos de barras sobre m√©tricas estructurales (grado, frecuencia de relaciones), que son las representaciones adecuadas para este tipo de datos\n"
      ],
      "metadata": {
        "id": "s6pcdio2j1L4"
      },
      "id": "s6pcdio2j1L4"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "def univariate_eda_kg(dataset_dir, dataset_name, top_n=10):\n",
        "    \"\"\"\n",
        "    Performs univariate EDA for a knowledge graph dataset.\n",
        "    Assumes train.txt, valid.txt, test.txt with (h, r, t).\n",
        "    \"\"\"\n",
        "\n",
        "    all_triples = []\n",
        "\n",
        "    # Load all splits\n",
        "    for split in [\"train\", \"valid\", \"test\"]:\n",
        "        path = os.path.join(dataset_dir, dataset_name, f\"{split}.txt\")\n",
        "        if os.path.exists(path):\n",
        "            df = pd.read_csv(\n",
        "                path,\n",
        "                sep=None,\n",
        "                engine=\"python\",\n",
        "                header=None,\n",
        "                on_bad_lines=\"skip\"\n",
        "            )\n",
        "            if df.shape[1] >= 3:\n",
        "                all_triples.append(df.iloc[:, :3])\n",
        "\n",
        "    if not all_triples:\n",
        "        print(f\"No data found for {dataset_name}\")\n",
        "        return\n",
        "\n",
        "    df = pd.concat(all_triples, ignore_index=True)\n",
        "    df.columns = [\"head\", \"relation\", \"tail\"]\n",
        "\n",
        "    # --- Metric 1: Entity degree (in + out) ---\n",
        "    degree_counter = Counter(df[\"head\"]) + Counter(df[\"tail\"])\n",
        "    degrees = list(degree_counter.values())\n",
        "\n",
        "    # --- Metric 2: Triples per entity ---\n",
        "    triples_per_entity = degrees  # same count interpretation here\n",
        "\n",
        "    # --- Metric 3: Relation frequencies ---\n",
        "    relation_freq = Counter(df[\"relation\"])\n",
        "\n",
        "    # --- Metric 4: Top-N hubs ---\n",
        "    top_hubs = degree_counter.most_common(top_n)\n",
        "\n",
        "    # ================== PLOTS ==================\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.hist(degrees, bins=50)\n",
        "    plt.title(f\"{dataset_name} ‚Äì Distribuci√≥n del grado de entidades\")\n",
        "    plt.xlabel(\"Grado\")\n",
        "    plt.ylabel(\"Frecuencia\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.hist(triples_per_entity, bins=50)\n",
        "    plt.title(f\"{dataset_name} ‚Äì Distribuci√≥n de tripletas por entidad\")\n",
        "    plt.xlabel(\"N√∫mero de tripletas\")\n",
        "    plt.ylabel(\"Frecuencia\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    rel_df = pd.DataFrame(\n",
        "        relation_freq.most_common(top_n),\n",
        "        columns=[\"Relaci√≥n\", \"Frecuencia\"]\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    plt.bar(rel_df[\"Relaci√≥n\"], rel_df[\"Frecuencia\"])\n",
        "    plt.title(f\"{dataset_name} ‚Äì Frecuencia de relaciones (Top-{top_n})\")\n",
        "    plt.xlabel(\"Relaci√≥n\")\n",
        "    plt.ylabel(\"Frecuencia\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    hubs_df = pd.DataFrame(top_hubs, columns=[\"Entidad\", \"Grado\"])\n",
        "\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    plt.bar(hubs_df[\"Entidad\"], hubs_df[\"Grado\"])\n",
        "    plt.title(f\"{dataset_name} ‚Äì Top-{top_n} entidades con mayor grado\")\n",
        "    plt.xlabel(\"Entidad\")\n",
        "    plt.ylabel(\"Grado\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "dataset_names = [d for d in os.listdir(FINAL_DATA_DIR) if os.path.isdir(os.path.join(FINAL_DATA_DIR, d))]\n",
        "print(\"Datasets:\")\n",
        "print(dataset_names)\n"
      ],
      "metadata": {
        "id": "s8nKTedgOAZl"
      },
      "id": "s8nKTedgOAZl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "univariate_eda_kg(FINAL_DATA_DIR, dataset_name=\"WN18RR\", top_n=10)"
      ],
      "metadata": {
        "id": "4BD1iU8yOq4p"
      },
      "id": "4BD1iU8yOq4p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "univariate_eda_kg(FINAL_DATA_DIR, dataset_name=\"FB15k-237\", top_n=10)"
      ],
      "metadata": {
        "id": "M1tlFeRXQJF5"
      },
      "id": "M1tlFeRXQJF5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "univariate_eda_kg(FINAL_DATA_DIR, dataset_name=\"FB13\", top_n=10)"
      ],
      "metadata": {
        "id": "TK0Snxl-QJfI"
      },
      "id": "TK0Snxl-QJfI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "univariate_eda_kg(FINAL_DATA_DIR, dataset_name=\"CoDEx-M\", top_n=10)"
      ],
      "metadata": {
        "id": "f6L5BQWXQJ4x"
      },
      "id": "f6L5BQWXQJ4x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "univariate_eda_kg(FINAL_DATA_DIR, dataset_name=\"WN11\", top_n=10)"
      ],
      "metadata": {
        "id": "b45pFhVSQKTD"
      },
      "id": "b45pFhVSQKTD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An√°lisis bi/multivariante"
      ],
      "metadata": {
        "id": "asdVUGEoj2BA"
      },
      "id": "asdVUGEoj2BA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Justificar que no tenemos multiples variables, solo tripletas.\n"
      ],
      "metadata": {
        "id": "ly-3Mhx7j4GP"
      },
      "id": "ly-3Mhx7j4GP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento"
      ],
      "metadata": {
        "id": "kQZoRSqDj5Iy"
      },
      "id": "kQZoRSqDj5Iy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordNet18RR a triplets\n"
      ],
      "metadata": {
        "id": "pAJ3l3Pvj7Du"
      },
      "id": "pAJ3l3Pvj7Du"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones"
      ],
      "metadata": {
        "id": "Xvp7BSrEj8Zf"
      },
      "id": "Xvp7BSrEj8Zf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se incluyen las ideas/observaciones clave, asegur√°ndose de responder todas las preguntas propuestas que apliquen al proyecto.\n"
      ],
      "metadata": {
        "id": "_mCYpxCaj-1M"
      },
      "id": "_mCYpxCaj-1M"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CNy-8gEVJju-",
        "5573ff36",
        "b809ee6d"
      ],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}