{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "746008f4",
   "metadata": {},
   "source": [
    "# **Maestría en Inteligencia Artificial Aplicada**\n",
    "\n",
    "## **Curso: Proyecto Integrador**\n",
    "\n",
    "### Tecnológico de Monterrey\n",
    "\n",
    "\n",
    "## **Actividad de la Semana 02**\n",
    "### **EDA de los conjuntos de datos seleccionados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07094be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING DATA LOADING ---\n",
      "\n",
      "Processing CoDEx-M...\n",
      "Found local: ./data\\CoDEx-M\\train.txt\n",
      "Found local: ./data\\CoDEx-M\\valid.txt\n",
      "Found local: ./data\\CoDEx-M\\test.txt\n",
      "  -> Loaded CoDEx-M Train: 185584 triplets\n",
      "  -> Sample: ['Q108946' 'P161' 'Q39792']\n",
      "\n",
      "Processing WN11...\n",
      "Downloading ./data\\WN11\\train.txt...\n",
      "  -> Failed to download https://github.com/KGCompletion/TransL/tree/master/WN11train.txt: 404 Client Error: Not Found for url: https://github.com/KGCompletion/TransL/tree/master/WN11train.txt\n",
      "Error reading train.txt: [Errno 2] No such file or directory: './data\\\\WN11\\\\train.txt'\n",
      "Downloading ./data\\WN11\\valid.txt...\n",
      "  -> Failed to download https://github.com/KGCompletion/TransL/tree/master/WN11valid.txt: 404 Client Error: Not Found for url: https://github.com/KGCompletion/TransL/tree/master/WN11valid.txt\n",
      "Error reading valid.txt: [Errno 2] No such file or directory: './data\\\\WN11\\\\valid.txt'\n",
      "Downloading ./data\\WN11\\test.txt...\n",
      "  -> Failed to download https://github.com/KGCompletion/TransL/tree/master/WN11test.txt: 404 Client Error: Not Found for url: https://github.com/KGCompletion/TransL/tree/master/WN11test.txt\n",
      "Error reading test.txt: [Errno 2] No such file or directory: './data\\\\WN11\\\\test.txt'\n",
      "\n",
      "Processing FB13...\n",
      "Downloading ./data\\FB13\\train.txt...\n",
      "  -> Failed to download https://github.com/KGCompletion/TransL/tree/master/FB13train.txt: 404 Client Error: Not Found for url: https://github.com/KGCompletion/TransL/tree/master/FB13train.txt\n",
      "Error reading train.txt: [Errno 2] No such file or directory: './data\\\\FB13\\\\train.txt'\n",
      "Downloading ./data\\FB13\\valid.txt...\n",
      "  -> Failed to download https://github.com/KGCompletion/TransL/tree/master/FB13valid.txt: 404 Client Error: Not Found for url: https://github.com/KGCompletion/TransL/tree/master/FB13valid.txt\n",
      "Error reading valid.txt: [Errno 2] No such file or directory: './data\\\\FB13\\\\valid.txt'\n",
      "Downloading ./data\\FB13\\test.txt...\n",
      "  -> Failed to download https://github.com/KGCompletion/TransL/tree/master/FB13test.txt: 404 Client Error: Not Found for url: https://github.com/KGCompletion/TransL/tree/master/FB13test.txt\n",
      "Error reading test.txt: [Errno 2] No such file or directory: './data\\\\FB13\\\\test.txt'\n",
      "\n",
      "--- READY FOR EDA ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "\n",
    "# --- CONFIGURATION (CORRECTED URLS) ---\n",
    "DATA_DIR = \"./data\"\n",
    "DATASETS = {\n",
    "    \"CoDEx-M\": {\n",
    "        # Corrected path: includes 'triples' folder\n",
    "        \"base_url\": \"https://raw.githubusercontent.com/tsafavi/codex/master/data/triples/codex-m/\",\n",
    "        \"files\": [\"train.txt\", \"valid.txt\", \"test.txt\"]\n",
    "    },\n",
    "    \"WN11\": {\n",
    "        \"base_url\": \"https://github.com/KGCompletion/TransL/tree/master/WN11\",\n",
    "        \"files\": [\"train.txt\", \"valid.txt\", \"test.txt\"]\n",
    "    },\n",
    "    \"FB13\": {\n",
    "        \"base_url\": \"https://github.com/KGCompletion/TransL/tree/master/FB13\",\n",
    "        \"files\": [\"train.txt\", \"valid.txt\", \"test.txt\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "def download_file(url, save_path):\n",
    "    \"\"\"Downloads a file from a URL if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(save_path):\n",
    "        print(f\"Downloading {save_path}...\")\n",
    "        try:\n",
    "            r = requests.get(url)\n",
    "            r.raise_for_status()\n",
    "            with open(save_path, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "            print(f\"  -> Success!\")\n",
    "        except Exception as e:\n",
    "            print(f\"  -> Failed to download {url}: {e}\")\n",
    "    else:\n",
    "        print(f\"Found local: {save_path}\")\n",
    "\n",
    "def load_dataset(name, config):\n",
    "    \"\"\"Loads train/valid/test into a dictionary of DataFrames.\"\"\"\n",
    "    path = os.path.join(DATA_DIR, name)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    dfs = {}\n",
    "    for file_name in config[\"files\"]:\n",
    "        # 1. Download\n",
    "        url = config[\"base_url\"] + file_name\n",
    "        local_path = os.path.join(path, file_name)\n",
    "        download_file(url, local_path)\n",
    "        \n",
    "        # 2. Load to Pandas\n",
    "        # Try-catch to handle potential parsing issues\n",
    "        try:\n",
    "            # We use engine='python' and sep=None to auto-detect tab or space separators\n",
    "            # This makes it robust against different format standards\n",
    "            df = pd.read_csv(local_path, sep=None, engine='python', \n",
    "                             names=['head', 'relation', 'tail'], \n",
    "                             on_bad_lines='skip')\n",
    "            \n",
    "            # Clean up: Sometimes the last column has a 4th value (label 1/-1) in older datasets\n",
    "            # For EDA we usually just want the triplet. Let's check shape.\n",
    "            if df.shape[1] > 3:\n",
    "                df = df.iloc[:, :3]\n",
    "                df.columns = ['head', 'relation', 'tail']\n",
    "                \n",
    "            dfs[file_name.replace('.txt', '')] = df\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_name}: {e}\")\n",
    "            \n",
    "    return dfs\n",
    "\n",
    "# --- EXECUTION ---\n",
    "print(\"--- STARTING DATA LOADING ---\")\n",
    "kg_data = {}\n",
    "\n",
    "for ds_name, config in DATASETS.items():\n",
    "    print(f\"\\nProcessing {ds_name}...\")\n",
    "    kg_data[ds_name] = load_dataset(ds_name, config)\n",
    "    \n",
    "    # Quick Sanity Check\n",
    "    if 'train' in kg_data[ds_name]:\n",
    "        count = len(kg_data[ds_name]['train'])\n",
    "        print(f\"  -> Loaded {ds_name} Train: {count} triplets\")\n",
    "        print(f\"  -> Sample: {kg_data[ds_name]['train'].iloc[0].values}\")\n",
    "\n",
    "print(\"\\n--- READY FOR EDA ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda3bcb2",
   "metadata": {},
   "source": [
    "# 1. Macro-Estadisticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b457e0",
   "metadata": {},
   "source": [
    "Before diving deep, you need a high-level comparison table. This establishes the scale.\n",
    "\n",
    "    Metrics:\n",
    "        Total Entities \n",
    "        Total Relations \n",
    "        Total Triplets \n",
    "        Split distribution: Count of Train / Validation / Test triples.\n",
    "        Graph Density: (Is the graph sparse or dense?).\n",
    "        Average Degree: On average, how many connections does a node have?\n",
    "\n",
    "    Why it matters: Sparse graphs (like WordNet) are harder for GNNs (Graph Neural Networks) because there is less \"message passing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0810c012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found datasets: ['CoDEx-M', 'FB13', 'FB15k-237', 'WN11', 'WN18RR']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_b797e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b797e_level0_col0\" class=\"col_heading level0 col0\" >Entities (|E|)</th>\n",
       "      <th id=\"T_b797e_level0_col1\" class=\"col_heading level0 col1\" >Relations (|R|)</th>\n",
       "      <th id=\"T_b797e_level0_col2\" class=\"col_heading level0 col2\" >Triples (Total)</th>\n",
       "      <th id=\"T_b797e_level0_col3\" class=\"col_heading level0 col3\" >Train</th>\n",
       "      <th id=\"T_b797e_level0_col4\" class=\"col_heading level0 col4\" >Valid</th>\n",
       "      <th id=\"T_b797e_level0_col5\" class=\"col_heading level0 col5\" >Test</th>\n",
       "      <th id=\"T_b797e_level0_col6\" class=\"col_heading level0 col6\" >Avg Degree</th>\n",
       "      <th id=\"T_b797e_level0_col7\" class=\"col_heading level0 col7\" >Graph Density</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Dataset</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b797e_level0_row0\" class=\"row_heading level0 row0\" >CoDEx-M</th>\n",
       "      <td id=\"T_b797e_row0_col0\" class=\"data row0 col0\" >17050</td>\n",
       "      <td id=\"T_b797e_row0_col1\" class=\"data row0 col1\" >51</td>\n",
       "      <td id=\"T_b797e_row0_col2\" class=\"data row0 col2\" >206205</td>\n",
       "      <td id=\"T_b797e_row0_col3\" class=\"data row0 col3\" >185584</td>\n",
       "      <td id=\"T_b797e_row0_col4\" class=\"data row0 col4\" >10310</td>\n",
       "      <td id=\"T_b797e_row0_col5\" class=\"data row0 col5\" >10311</td>\n",
       "      <td id=\"T_b797e_row0_col6\" class=\"data row0 col6\" >12.09</td>\n",
       "      <td id=\"T_b797e_row0_col7\" class=\"data row0 col7\" >0.000709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b797e_level0_row1\" class=\"row_heading level0 row1\" >FB13</th>\n",
       "      <td id=\"T_b797e_row1_col0\" class=\"data row1 col0\" >75043</td>\n",
       "      <td id=\"T_b797e_row1_col1\" class=\"data row1 col1\" >13</td>\n",
       "      <td id=\"T_b797e_row1_col2\" class=\"data row1 col2\" >375514</td>\n",
       "      <td id=\"T_b797e_row1_col3\" class=\"data row1 col3\" >316232</td>\n",
       "      <td id=\"T_b797e_row1_col4\" class=\"data row1 col4\" >11816</td>\n",
       "      <td id=\"T_b797e_row1_col5\" class=\"data row1 col5\" >47466</td>\n",
       "      <td id=\"T_b797e_row1_col6\" class=\"data row1 col6\" >5.00</td>\n",
       "      <td id=\"T_b797e_row1_col7\" class=\"data row1 col7\" >0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b797e_level0_row2\" class=\"row_heading level0 row2\" >FB15k-237</th>\n",
       "      <td id=\"T_b797e_row2_col0\" class=\"data row2 col0\" >14541</td>\n",
       "      <td id=\"T_b797e_row2_col1\" class=\"data row2 col1\" >237</td>\n",
       "      <td id=\"T_b797e_row2_col2\" class=\"data row2 col2\" >310116</td>\n",
       "      <td id=\"T_b797e_row2_col3\" class=\"data row2 col3\" >272115</td>\n",
       "      <td id=\"T_b797e_row2_col4\" class=\"data row2 col4\" >17535</td>\n",
       "      <td id=\"T_b797e_row2_col5\" class=\"data row2 col5\" >20466</td>\n",
       "      <td id=\"T_b797e_row2_col6\" class=\"data row2 col6\" >21.33</td>\n",
       "      <td id=\"T_b797e_row2_col7\" class=\"data row2 col7\" >0.001467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b797e_level0_row3\" class=\"row_heading level0 row3\" >WN11</th>\n",
       "      <td id=\"T_b797e_row3_col0\" class=\"data row3 col0\" >38588</td>\n",
       "      <td id=\"T_b797e_row3_col1\" class=\"data row3 col1\" >11</td>\n",
       "      <td id=\"T_b797e_row3_col2\" class=\"data row3 col2\" >138887</td>\n",
       "      <td id=\"T_b797e_row3_col3\" class=\"data row3 col3\" >112581</td>\n",
       "      <td id=\"T_b797e_row3_col4\" class=\"data row3 col4\" >5218</td>\n",
       "      <td id=\"T_b797e_row3_col5\" class=\"data row3 col5\" >21088</td>\n",
       "      <td id=\"T_b797e_row3_col6\" class=\"data row3 col6\" >3.60</td>\n",
       "      <td id=\"T_b797e_row3_col7\" class=\"data row3 col7\" >0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b797e_level0_row4\" class=\"row_heading level0 row4\" >WN18RR</th>\n",
       "      <td id=\"T_b797e_row4_col0\" class=\"data row4 col0\" >40943</td>\n",
       "      <td id=\"T_b797e_row4_col1\" class=\"data row4 col1\" >11</td>\n",
       "      <td id=\"T_b797e_row4_col2\" class=\"data row4 col2\" >93003</td>\n",
       "      <td id=\"T_b797e_row4_col3\" class=\"data row4 col3\" >86835</td>\n",
       "      <td id=\"T_b797e_row4_col4\" class=\"data row4 col4\" >3034</td>\n",
       "      <td id=\"T_b797e_row4_col5\" class=\"data row4 col5\" >3134</td>\n",
       "      <td id=\"T_b797e_row4_col6\" class=\"data row4 col6\" >2.27</td>\n",
       "      <td id=\"T_b797e_row4_col7\" class=\"data row4 col7\" >0.000055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20a88273bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def calculate_macro_stats(base_dir=\"./data\"):\n",
    "    stats_list = []\n",
    "    \n",
    "    # Get all subdirectories in ./data\n",
    "    dataset_names = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    \n",
    "    print(f\"Found datasets: {dataset_names}\")\n",
    "    \n",
    "    for name in dataset_names:\n",
    "        ds_path = os.path.join(base_dir, name)\n",
    "        \n",
    "        # Initialize sets to track unique entities and relations across ALL splits\n",
    "        all_entities = set()\n",
    "        all_relations = set()\n",
    "        \n",
    "        split_counts = {'train': 0, 'valid': 0, 'test': 0}\n",
    "        total_triples = 0\n",
    "        \n",
    "        # Process each split\n",
    "        for split in ['train', 'valid', 'test']:\n",
    "            file_path = os.path.join(ds_path, f\"{split}.txt\")\n",
    "            \n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    # Robust loading: handles comma, tab, or space separators automatically\n",
    "                    df = pd.read_csv(file_path, sep=None, engine='python', header=None, on_bad_lines='skip')\n",
    "                    \n",
    "                    # Ensure we have at least 3 columns\n",
    "                    if df.shape[1] >= 3:\n",
    "                        # Extract Head, Relation, Tail (assuming 0=Head, 1=Rel, 2=Tail based on standard formats)\n",
    "                        # NOTE: Some datasets are H, T, R. Others H, R, T. \n",
    "                        # For simply COUNTING entities/relations, column order doesn't matter much \n",
    "                        # as long as we grab the entities and relation columns.\n",
    "                        # We assume standard H, R, T or H, T, R. We'll take col 1 as relation mostly.\n",
    "                        # But to be safe for counts, we treat the \"middle\" as relation usually, \n",
    "                        # or we can check unique counts to guess, but let's stick to standard H, R, T logic here.\n",
    "                        \n",
    "                        heads = df.iloc[:, 0].astype(str)\n",
    "                        rels = df.iloc[:, 1].astype(str)\n",
    "                        tails = df.iloc[:, 2].astype(str)\n",
    "                        \n",
    "                        # Update counts\n",
    "                        count = len(df)\n",
    "                        split_counts[split] = count\n",
    "                        total_triples += count\n",
    "                        \n",
    "                        # Update Unique Sets\n",
    "                        all_entities.update(heads.unique())\n",
    "                        all_entities.update(tails.unique())\n",
    "                        all_relations.update(rels.unique())\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {name}/{split}: {e}\")\n",
    "        \n",
    "        # Calculate Derived Metrics\n",
    "        num_entities = len(all_entities)\n",
    "        num_relations = len(all_relations)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if num_entities > 0:\n",
    "            # Avg Degree = Triples / Entities\n",
    "            avg_degree = total_triples / num_entities\n",
    "            \n",
    "            # Density = Triples / (Entities * (Entities - 1))  [Directed Graph]\n",
    "            density = total_triples / (num_entities * (num_entities - 1))\n",
    "        else:\n",
    "            avg_degree = 0\n",
    "            density = 0\n",
    "            \n",
    "        stats_list.append({\n",
    "            'Dataset': name,\n",
    "            'Entities (|E|)': num_entities,\n",
    "            'Relations (|R|)': num_relations,\n",
    "            'Triples (Total)': total_triples,\n",
    "            'Train': split_counts['train'],\n",
    "            'Valid': split_counts['valid'],\n",
    "            'Test': split_counts['test'],\n",
    "            'Avg Degree': round(avg_degree, 4),\n",
    "            'Graph Density': density # Keep distinct to see scientific notation if needed\n",
    "        })\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_stats = pd.DataFrame(stats_list)\n",
    "    \n",
    "    # Set Dataset as index\n",
    "    if not df_stats.empty:\n",
    "        df_stats = df_stats.set_index('Dataset')\n",
    "        \n",
    "    return df_stats\n",
    "\n",
    "# --- EXECUTE ---\n",
    "df_macro_stats = calculate_macro_stats()\n",
    "\n",
    "# Display formatted table\n",
    "# We use style to format the Density column because it is usually very small (e.g., 0.0001)\n",
    "display(df_macro_stats.style.format({\n",
    "    'Graph Density': '{:.6f}',\n",
    "    'Avg Degree': '{:.2f}'\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ead0b",
   "metadata": {},
   "source": [
    "# 2. Análisis de relaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec45c66",
   "metadata": {},
   "source": [
    "Knowledge Graphs rarely have balanced relations. Some relations are incredibly common (e.g., gender), others are rare.\n",
    "\n",
    "    Analysis:\n",
    "\n",
    "        Frequency Histogram: Plot the count of triplets per relation.\n",
    "\n",
    "        Head vs. Tail Relations: Identify which relations make up the top 50% of the data.\n",
    "\n",
    "    Relevance to Project: If you are extrapolating, it is easier to predict a common relation for a new node than a rare one. If CoDEx-M is more balanced than FB13, your model might perform more consistently there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d32edbb",
   "metadata": {},
   "source": [
    "# 3. Node Degree Distribution (The \"Hubs\" Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e07e1",
   "metadata": {},
   "source": [
    "This is crucial for graph embeddings. Real-world graphs follow a Power Law (Long Tail).\n",
    "\n",
    "    Analysis:\n",
    "\n",
    "        Plot the distribution of node degrees (log-log scale).\n",
    "\n",
    "        Identify \"Hubs\" (super-connected nodes, like \"USA\" in Freebase) vs. \"Tail Entities\" (nodes with 1 or 2 connections).\n",
    "\n",
    "    Relevance to Extrapolation (CRITICAL):\n",
    "\n",
    "        Hypothesis: New entities (unseen) that connect to Hubs are easier to embed because the Hub has a stable, rich representation. New entities that connect only to other sparse nodes are extremely hard to predict.\n",
    "\n",
    "        Action: Check what percentage of entities in WN11/FB13 have a degree <3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e336003",
   "metadata": {},
   "source": [
    "# 4. Connectivity Patterns (The \"Structure\" Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd3b22",
   "metadata": {},
   "source": [
    "Since WN11 comes from WordNet (hierarchy) and FB13 from Freebase (facts), their shapes are different.\n",
    "\n",
    "    Analysis:\n",
    "\n",
    "        Clustering Coefficient: Do neighbors of a node know each other? (Triangles).\n",
    "\n",
    "        Diameter / Average Path Length: How many hops to get from node A to node B?\n",
    "\n",
    "    Relevance:\n",
    "\n",
    "        WN11: Likely has low clustering (tree structure, hierarchies don't form many triangles).\n",
    "\n",
    "        FB13/CoDEx: Likely higher clustering (social networks, movies). GNNs behave differently on trees vs. clustered graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75828eb",
   "metadata": {},
   "source": [
    "# 5. The \"Inductive Bias\" Analysis (The Most Important for You)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa95e30",
   "metadata": {},
   "source": [
    "Since your project is about unseen entities, you must analyze the \"gap\" between Train and Test. Standard EDAs don't do this, but you must.\n",
    "\n",
    "A. Unseen Entity Ratio (if using standard splits):\n",
    "\n",
    "    In the Test set, how many entities are strictly new?\n",
    "\n",
    "    (Note: If you use the specific benchmarks from Hamaguchi et al. or GraIL, this is 100%, but you should verify it).\n",
    "\n",
    "B. \"Connectivity to Known\" Ratio:\n",
    "\n",
    "    For the triples in the Test set (involving new entities), who are they connected to?\n",
    "\n",
    "        Scenario A: New Entity\n",
    "\n",
    "        Old Entity (Seen in Train).\n",
    "\n",
    "        Scenario B: New Entity\n",
    "\n",
    "\n",
    "        New Entity.\n",
    "\n",
    "    Why it matters:\n",
    "\n",
    "        Scenario A is \"Transductive-Inductive\". Easier. The model uses the Old Entity as an anchor.\n",
    "\n",
    "        Scenario B is \"Fully Inductive\". Extremely hard. If a new node only connects to another new node, the model is \"blind.\"\n",
    "\n",
    "    The Chart: A bar chart showing the % of test triples that connect to Seen vs. Unseen nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ed25f",
   "metadata": {},
   "source": [
    "# 6. Text/Semantics Analysis (Optional but recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbbd82f",
   "metadata": {},
   "source": [
    "Since you mentioned using \"descriptions\" or \"external info\" for extrapolation.\n",
    "\n",
    "    Analysis: Calculate the average length of text descriptions for entities in CoDEx vs FB13.\n",
    "\n",
    "    Relevance: If FB13 has rich text and WN11 has short definitions, your text-based extrapolation model might work better on FB13."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
