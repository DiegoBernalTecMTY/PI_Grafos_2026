# INGRAM: Inductive Knowledge Graph Embedding via Relation Graphs

**ImplementaciÃ³n basada en:** Lee et al., 2023 (ICML)  
**Paper:** [INGRAM: Inductive Knowledge Graph Embedding via Relation Graphs](https://arxiv.org/abs/2305.19987)

## ğŸ“‹ DescripciÃ³n

INGRAM es el primer modelo de Knowledge Graph Embedding que puede generar embeddings de **relaciones nuevas** en tiempo de inferencia, ademÃ¡s de entidades nuevas. Esto lo hace ideal para escenarios de **Zero-Shot Relation Learning**.

### Problema que resuelve

Los modelos tradicionales de KG completion fallan cuando aparecen relaciones no vistas durante el entrenamiento:

- âŒ **GraIL, CoMPILE, SNRI**: Solo manejan entidades nuevas
- âŒ **TransE, RotatE, DistMult**: Requieren todas las relaciones en training
- âœ… **INGRAM**: Maneja relaciones Y entidades completamente nuevas

### InnovaciÃ³n clave: Grafo de Relaciones

INGRAM construye un grafo donde:
- **Nodos** = Relaciones del KG
- **Aristas** = Afinidad entre relaciones (basada en co-ocurrencia de entidades)

Esto permite que relaciones nuevas se representen como **combinaciÃ³n ponderada** de relaciones conocidas similares.

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INGRAM Architecture                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. RELATION GRAPH BUILDER (SecciÃ³n 4)                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚     â”‚ Input: Tripletas (h, r, t)              â”‚            â”‚
â”‚     â”‚ Output: Matriz A (afinidad relaciones)   â”‚            â”‚
â”‚     â”‚                                          â”‚            â”‚
â”‚     â”‚ Proceso:                                 â”‚            â”‚
â”‚     â”‚  â€¢ Eh[i,j] = freq(entidad_i como head de rel_j)      â”‚
â”‚     â”‚  â€¢ Et[i,j] = freq(entidad_i como tail de rel_j)      â”‚
â”‚     â”‚  â€¢ Ah = Eh^T @ Dh^(-2) @ Eh              â”‚            â”‚
â”‚     â”‚  â€¢ At = Et^T @ Dt^(-2) @ Et              â”‚            â”‚
â”‚     â”‚  â€¢ A = Ah + At (con self-loops)          â”‚            â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                         â†“                                    â”‚
â”‚  2. RELATION-LEVEL AGGREGATION (SecciÃ³n 5.1)                â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚     â”‚ L capas de atenciÃ³n multi-head           â”‚            â”‚
â”‚     â”‚                                          â”‚            â”‚
â”‚     â”‚ Para cada relaciÃ³n r_i:                  â”‚            â”‚
â”‚     â”‚   z_i^(l+1) = Ïƒ(Î£ Î±_ij W^(l) z_j^(l))   â”‚            â”‚
â”‚     â”‚                                          â”‚            â”‚
â”‚     â”‚ donde Î±_ij incluye:                      â”‚            â”‚
â”‚     â”‚   â€¢ AtenciÃ³n local (GAT-style)           â”‚            â”‚
â”‚     â”‚   â€¢ Peso de afinidad global c_s(i,j)     â”‚            â”‚
â”‚     â”‚                                          â”‚            â”‚
â”‚     â”‚ Novedad: Binning de afinidad             â”‚            â”‚
â”‚     â”‚   s(i,j) = bin basado en rank(A[i,j])   â”‚            â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                         â†“                                    â”‚
â”‚  3. ENTITY-LEVEL AGGREGATION (SecciÃ³n 5.2)                  â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚     â”‚ LÌ‚ capas de atenciÃ³n multi-head           â”‚            â”‚
â”‚     â”‚                                          â”‚            â”‚
â”‚     â”‚ Para cada entidad v_i:                   â”‚            â”‚
â”‚     â”‚   h_i^(l+1) = Ïƒ(Î²_ii Wc[h_i || zÌ„_i] +   â”‚            â”‚
â”‚     â”‚                  Î£ Î²_ijk Wc[h_j || z_k]) â”‚            â”‚
â”‚     â”‚                                          â”‚            â”‚
â”‚     â”‚ ExtensiÃ³n de GATv2:                      â”‚            â”‚
â”‚     â”‚   â€¢ Incorpora vectores de relaciÃ³n       â”‚            â”‚
â”‚     â”‚   â€¢ zÌ„_i = promedio de relaciones adj.    â”‚            â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                         â†“                                    â”‚
â”‚  4. SCORING FUNCTION (SecciÃ³n 5.3)                          â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚     â”‚ Variante de DistMult:                    â”‚            â”‚
â”‚     â”‚                                          â”‚            â”‚
â”‚     â”‚   f(h, r, t) = h^T diag(W z_r) t        â”‚            â”‚
â”‚     â”‚                                          â”‚            â”‚
â”‚     â”‚ donde:                                   â”‚            â”‚
â”‚     â”‚   â€¢ h, t: entity embeddings finales      â”‚            â”‚
â”‚     â”‚   â€¢ z_r: relation embedding final        â”‚            â”‚
â”‚     â”‚   â€¢ W: matriz de transformaciÃ³n          â”‚            â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Componentes Clave

### 1. Relation Graph Builder

**Paper SecciÃ³n 4**

Construye la matriz de adyacencia A del grafo de relaciones:

```python
# Matrices de frecuencia
Eh[i, j] = frecuencia de entidad_i como head de relaciÃ³n_j
Et[i, j] = frecuencia de entidad_i como tail de relaciÃ³n_j

# NormalizaciÃ³n por grado de entidad
Dh[i, i] = Î£_j Eh[i, j]  # Grado de entidad i como head
Dt[i, i] = Î£_j Et[i, j]  # Grado de entidad i como tail

# Afinidad entre relaciones
Ah = Eh^T @ Dh^(-2) @ Eh  # Afinidad vÃ­a heads
At = Et^T @ Dt^(-2) @ Et  # Afinidad vÃ­a tails

# Matriz final
A = Ah + At + I  # I = self-loops
```

**IntuiciÃ³n:** Dos relaciones tienen alta afinidad si comparten muchas entidades frecuentemente.

**Ejemplo:**
```
RelaciÃ³n "BornIn" y "LivesIn" â†’ Alta afinidad (comparten personas/lugares)
RelaciÃ³n "ActedIn" y "TimeZone" â†’ Baja afinidad (dominios diferentes)
```

### 2. Relation-Level Aggregation

**Paper Ecuaciones 1-3**

Actualiza representaciones de relaciones mediante atenciÃ³n:

```python
# Para cada relaciÃ³n r_i
z_i^(l+1) = Ïƒ(Î£_{r_j âˆˆ N_i} Î±_ij W^(l) z_j^(l))

# Coeficiente de atenciÃ³n
Î±_ij = softmax(y^(l) Ïƒ(P^(l) [z_i || z_j]) + c_s(i,j))
       \_________________________________________/   \______/
                 AtenciÃ³n local (GATv2)          Peso de afinidad global

# Binning de afinidad
s(i,j) = âŒŠrank(A[i,j]) Ã— B / nnz(A)âŒ‹
```

**Diferencia clave vs GAT/GATv2:**
- GAT: Solo atenciÃ³n local
- INGRAM: AtenciÃ³n local + **pesos de afinidad global** (c_s(i,j))

El binning permite aprender B parÃ¡metros distintos para diferentes niveles de afinidad:
- c_1: Para relaciones muy afines (rank bajo)
- c_B: Para relaciones poco afines (rank alto)

### 3. Entity-Level Aggregation

**Paper EcuaciÃ³n 4**

Extiende GATv2 incorporando vectores de relaciÃ³n:

```python
# Para cada entidad v_i
h_i^(l+1) = Ïƒ(Î²_ii Wc^(l)[h_i || zÌ„_i] + 
              Î£_{v_j âˆˆ N_i} Î£_{r_k âˆˆ R_ji} Î²_ijk Wc^(l)[h_j || z_k])

# zÌ„_i = promedio de relaciones adyacentes a v_i
zÌ„_i = (1/|N_i|) Î£_{v_j âˆˆ N_i} Î£_{r_k âˆˆ R_ji} z_k^(L)

# AtenciÃ³n
Î²_ijk = softmax(Å·^(l) Ïƒ(PÌ‚^(l) [h_i || h_j || z_k]))
```

**ExtensiÃ³n de GATv2:**
- GATv2: Agrega solo entidades vecinas
- INGRAM: Agrega entidades + **relaciones que las conectan**

Esto es crucial porque relaciones distintas tienen semÃ¡nticas diferentes:
- `(Obama, BornIn, Hawaii)` vs `(Obama, PresidentOf, USA)`

### 4. Training Regime: DivisiÃ³n DinÃ¡mica

**Paper SecciÃ³n 5.4**

Estrategia clave para generalizaciÃ³n:

```python
Para cada Ã©poca:
    1. Re-split Etr en Ftr y Ttr (ratio 3:1)
       Restricciones:
       â€¢ Ftr contiene Ã¡rbol de expansiÃ³n mÃ­nimo
       â€¢ Ftr cubre todas las relaciones
    
    2. Re-inicializar features (Glorot init)
       â€¢ entity_features â† Xavier_uniform(num_entities, dÌ‚)
       â€¢ relation_features â† Xavier_uniform(num_relations, d)
    
    3. Entrenar en Ttr con loss:
       L = Î£ max(0, Î³ - f(pos) + f(neg))
```

**Â¿Por quÃ© funciona?**
- **DivisiÃ³n dinÃ¡mica**: Evita memorizar configuraciones especÃ­ficas
- **Re-inicializaciÃ³n**: Aprende a generalizar desde features aleatorios

â†’ En inferencia, puede manejar features de relaciones completamente nuevas

## ğŸ“Š Resultados del Paper

ComparaciÃ³n en datasets con **100% relaciones nuevas** (mÃ¡s desafiante):

| MÃ©todo | NL-100 MRR | WK-100 MRR | FB-100 MRR |
|--------|------------|------------|------------|
| GraIL | 0.135 | - | - |
| RMPI | 0.220 | - | - |
| RED-GNN | 0.212 | 0.096 | 0.121 |
| NBFNet | 0.096 | 0.014 | 0.072 |
| **INGRAM** | **0.309** â†‘ | **0.107** â†‘ | **0.223** â†‘ |

**Tiempo de entrenamiento** (NL-100):
- RMPI: 52 horas
- **INGRAM: 15 minutos** (200Ã— mÃ¡s rÃ¡pido)

## ğŸš€ Uso

### InstalaciÃ³n

```bash
pip install torch numpy pandas tqdm scikit-learn matplotlib seaborn
```

### Entrenamiento BÃ¡sico

```python
from ingram_model import INGRAM, INGRAMTrainer

# Crear modelo
model = INGRAM(
    num_entities=1000,
    num_relations=50,
    entity_dim=32,
    relation_dim=32,
    entity_hidden_dim=128,
    relation_hidden_dim=64,
    num_relation_layers=2,
    num_entity_layers=3
)

# Entrenar
trainer = INGRAMTrainer(model, lr=0.001, margin=1.5)
loss = trainer.train_epoch(triplets, num_entities, num_relations)
```

### Inferencia con Relaciones Nuevas

```python
# Generar embeddings (relaciones pueden ser nuevas)
entity_emb, relation_emb = model(inference_triplets)

# Scoring
scores = model.score(heads, rels, tails, entity_emb, relation_emb)
```

### Script Completo

```bash
python train_ingram.py \
    --dataset CoDEx-M \
    --mode inductive \
    --split NL-25 \
    --epochs 10000 \
    --val_every 200 \
    --lr 0.001 \
    --margin 1.5
```

## ğŸ”¬ IntegraciÃ³n con Scripts Provistos

### KGDataLoader

```python
from kg_dataloader import KGDataLoader

# Cargar datos
loader = KGDataLoader('CoDEx-M', mode='inductive', inductive_split='NL-25')
loader.load()

# Entrenar INGRAM
model = INGRAM(loader.num_entities, loader.num_relations, ...)
trainer = INGRAMTrainer(model)

for epoch in range(10000):
    loss = trainer.train_epoch(loader.train_data, ...)
```

### UnifiedKGScorer

```python
from unified_kg_scorer import UnifiedKGScorer

# Generar embeddings
entity_emb, relation_emb = model(triplets)
predict_fn = create_predict_fn(model, entity_emb, relation_emb)

# Evaluar
scorer = UnifiedKGScorer(device='cuda')

# Ranking metrics
ranking_metrics = scorer.evaluate_ranking(
    predict_fn, 
    test_triples=loader.test_data.numpy(),
    num_entities=loader.num_entities,
    k_values=[1, 3, 10]
)

# Classification metrics
class_metrics = scorer.evaluate_classification(
    predict_fn,
    valid_pos=loader.valid_data.numpy(),
    test_pos=loader.test_data.numpy(),
    num_entities=loader.num_entities
)

# Generar reporte PDF
scorer.export_report("INGRAM", "reporte_ingram.pdf")
```

## ğŸ“– Detalles de ImplementaciÃ³n

### Diferencias con el Paper

1. **AgregaciÃ³n de Relaciones (SecciÃ³n 5.1)**
   - Paper: ImplementaciÃ³n con sparse tensors
   - ImplementaciÃ³n: IteraciÃ³n explÃ­cita sobre relaciones (mÃ¡s clara para demo)
   - OptimizaciÃ³n futura: Usar torch_sparse para escalabilidad

2. **DivisiÃ³n DinÃ¡mica (SecciÃ³n 5.4)**
   - Paper: Minimum spanning tree exacto
   - ImplementaciÃ³n: BFS para conectividad (mÃ¡s simple)
   - Ambos garantizan: Grafo conexo + todas relaciones cubiertas

3. **Multi-Head Attention**
   - Implementado segÃºn Brody et al., 2022 (GATv2)
   - Resuelve "static attention" mencionado en el paper

### HiperparÃ¡metros Recomendados (del Paper)

**Dimensiones:**
- `entity_dim`, `relation_dim`: 32
- `entity_hidden_dim`: 128, 256
- `relation_hidden_dim`: 64, 128, 256

**Capas:**
- `num_relation_layers` (L): 1, 2, 3
- `num_entity_layers` (LÌ‚): 2, 3, 4

**AtenciÃ³n:**
- `num_relation_heads` (K): 8, 16
- `num_entity_heads` (KÌ‚): 8, 16
- `num_bins` (B): 1, 5, 10

**Entrenamiento:**
- `lr`: 0.0005, 0.001
- `margin` (Î³): 1.0, 1.5, 2.0, 2.5
- `epochs`: 10,000
- `val_every`: 200

**Mejor configuraciÃ³n (NL datasets):**
```python
model = INGRAM(
    entity_dim=32,
    relation_dim=32,
    entity_hidden_dim=256,
    relation_hidden_dim=64,
    num_relation_layers=2,
    num_entity_layers=3,
    num_relation_heads=8,
    num_entity_heads=8,
    num_bins=10
)
```

## ğŸ§ª Testing

Ejecutar test bÃ¡sico:

```bash
python ingram_model.py
```

Salida esperada:
```
================================================================================
INGRAM: Inductive Knowledge Graph Embedding via Relation Graphs
ImplementaciÃ³n basada en Lee et al., 2023 (ICML)
================================================================================

Dispositivo: cuda

Modelo creado con:
  - 100 entidades
  - 20 relaciones
  - XXX,XXX parÃ¡metros totales

  - 500 tripletas sintÃ©ticas generadas

Ejecutando forward pass...
  âœ“ Entity embeddings: torch.Size([100, 32])
  âœ“ Relation embeddings: torch.Size([20, 32])
  âœ“ Scores de prueba: tensor([...])

âœ“ Test bÃ¡sico completado exitosamente!
================================================================================
```

## ğŸ“š Referencias

**Paper principal:**
```bibtex
@inproceedings{lee2023ingram,
  title={INGRAM: Inductive Knowledge Graph Embedding via Relation Graphs},
  author={Lee, Jaejun and Chung, Chanyoung and Whang, Joyce Jiyoung},
  booktitle={Proceedings of the 40th International Conference on Machine Learning},
  year={2023}
}
```

**MÃ©todos relacionados:**
- GATv2: Brody et al., 2022 (resuelve static attention)
- GraIL: Teru et al., 2020 (subgraph reasoning, solo entidades nuevas)
- DistMult: Yang et al., 2015 (scoring function base)

## ğŸ¤ ComparaciÃ³n con Baselines

| CaracterÃ­stica | GraIL | RMPI | RED-GNN | INGRAM |
|---------------|-------|------|---------|--------|
| Entidades nuevas | âœ… | âœ… | âœ… | âœ… |
| Relaciones nuevas | âŒ | âŒ | âŒ | âœ… |
| Usa LLMs | âŒ | âŒ | âŒ | âŒ |
| Escalabilidad | Baja | Muy Baja | Media | Alta |
| Grafo de relaciones | âŒ | âŒ | âŒ | âœ… |
| DivisiÃ³n dinÃ¡mica | âŒ | âŒ | âŒ | âœ… |

## ğŸ’¡ Casos de Uso

1. **Knowledge Graphs Evolutivos**
   - AÃ±adir nuevas relaciones sin re-entrenar
   - Ejemplo: AÃ±adir "VacunadoCon" en un KG mÃ©dico

2. **Transfer Learning entre Dominios**
   - Entrenar en un dominio, inferir en otro
   - Ejemplo: Entrenar en Freebase, aplicar a Wikidata

3. **Few-Shot Learning**
   - Pocas muestras de relaciones nuevas
   - INGRAM puede interpolar desde relaciones similares

## ğŸ› Troubleshooting

**Out of Memory:**
```python
# Reducir dimensiones ocultas
entity_hidden_dim=128  # en lugar de 256
relation_hidden_dim=32  # en lugar de 64

# Reducir batch size
batch_size=64  # en lugar de 128
```

**Convergencia lenta:**
```python
# Aumentar learning rate
lr=0.002  # en lugar de 0.001

# Reducir margin
margin=1.0  # en lugar de 1.5
```

**Overfitting:**
```python
# Aumentar dropout
dropout=0.2  # en lugar de 0.1

# Asegurar divisiÃ³n dinÃ¡mica estÃ¡ activa
# (deberÃ­a estar por defecto)
```

## ğŸ“ TODO / Mejoras Futuras

- [ ] Soporte para grafos temporales (MTKGE extension)
- [ ] ImplementaciÃ³n con torch_sparse para grafos grandes
- [ ] Pre-entrenamiento con contrastive learning
- [ ] IntegraciÃ³n con HuggingFace Transformers
- [ ] Benchmarks en datasets oficiales (FB15k-237, WN18RR)
- [ ] VisualizaciÃ³n de embeddings de relaciones
- [ ] AnÃ¡lisis de interpretabilidad de pesos c_s(i,j)

## ğŸ“§ Contacto

Para preguntas sobre la implementaciÃ³n o el paper, consultar:
- Paper: https://arxiv.org/abs/2305.19987
- Repo oficial: https://github.com/bdi-lab/InGram

---

**ImplementaciÃ³n realizada para fines de investigaciÃ³n basada en:**  
Lee, J., Chung, C., & Whang, J. J. (2023). INGRAM: Inductive Knowledge Graph Embedding via Relation Graphs. *ICML 2023*.
