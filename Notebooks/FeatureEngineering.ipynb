{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "Mj6g11A-IjnJ",
        "7pI_W7KtImWJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering."
      ],
      "metadata": {
        "id": "WPj6EBAy0YEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idea principal: Debido a la naturaleza de la problemática de este proyecto no se “limpian” datos para facilitar el modelo, se manipulan para *dificultarlo* de forma controlada, es decir, inducir un escenario en el que los conjuntos de validación y prueba tienen datos no observados durante el entrenamiento."
      ],
      "metadata": {
        "id": "nTDZ41_5pXd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Transformación de datos crudos en variables útiles para el aprendizaje automático"
      ],
      "metadata": {
        "id": "-5G5cLK_0awH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este proyecto, los datos no corresponden a observaciones tabulares tradicionales, sino a un grafo de conocimiento representado mediante tripletas (h, r, t), donde h y t son entidades y r es una relación. En este contexto, no existen variables numéricas continuas sobre las cuales aplicar técnicas clásicas de ingeniería de características como escalamiento, discretización, transformaciones matemáticas o codificaciones one-hot.\n",
        "\n",
        "Sin embargo, el rol que tradicionalmente cumple la ingeniería de características es reformular los datos de entrada para que contengan la información relevante que permita abordar una problemática específica. Bajo esta interpretación, la etapa implementada en la libreta corresponde a una ingeniería de características estructural, donde los datos son transformados a nivel de grafo, no a nivel de atributo.\n",
        "\n",
        "En particular, se realizan operaciones controladas de particionado inductivo del conjunto de tripletas con dos objetivos distintos, alineados con los trabajos base del proyecto:\n",
        "\n",
        "* Escenario InGram: se generan conjuntos de validación y prueba que contienen relaciones completamente nuevas, ausentes del conjunto de entrenamiento. Esta transformación fuerza al modelo a aprender representaciones relacionales capaces de generalizar a relaciones no vistas.\n",
        "\n",
        "* Escenario OOKB (Out-Of-Knowledge-Base): se construyen conjuntos de validación y prueba que incluyen entidades nuevas, no presentes durante el entrenamiento, mientras que el conjunto de relaciones se mantiene constante.\n",
        "\n",
        "Estas operaciones no generan nuevas variables explícitas, pero modifican deliberadamente la distribución y composición del conjunto de datos, asegurando que la información disponible para el modelo sea consistente con un escenario inductivo. Por lo tanto, esta etapa cumple una función equivalente a la ingeniería de características clásica, ya que define qué información estructural está disponible para el aprendizaje y cuál debe ser inferida por el modelo. Todas las decisiones de particionado se justifican directamente por los supuestos experimentales de los papers de referencia."
      ],
      "metadata": {
        "id": "i9lR3OciqCQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B. Selección y extracción de características"
      ],
      "metadata": {
        "id": "eTpJ1WD_qjwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las técnicas clásicas de selección y extracción de características, como umbral de varianza, correlación, pruebas estadísticas (chi-cuadrado, ANOVA) o métodos de reducción de dimensionalidad (PCA, FA), no son aplicables en este problema, ya que dichas técnicas requieren un espacio de características explícito y generalmente numérico.\n",
        "\n",
        "En el caso de los grafos de conocimiento, las “características” no existen de forma directa, sino que emergen posteriormente como embeddings aprendidos por el modelo a partir de la estructura relacional del grafo. Por esta razón, cualquier intento de aplicar selección o extracción de características previa al entrenamiento sería conceptualmente incorrecto y metodológicamente inconsistente con el enfoque de knowledge graph embedding.\n",
        "\n",
        "No obstante, de manera análoga a la selección de características, la libreta implementa mecanismos de control estructural para reducir sesgos y evitar fugas de información (information leakage), tales como:\n",
        "\n",
        "* Exclusión estricta de relaciones nuevas del conjunto de entrenamiento (InGram).\n",
        "\n",
        "* Exclusión estricta de entidades nuevas del conjunto de entrenamiento (OOKB).\n",
        "\n",
        "* Separación explícita y reproducible de los conjuntos train / validation / test.\n",
        "\n",
        "Estas decisiones cumplen un rol equivalente al filtrado de características, ya que limitan la información disponible durante el entrenamiento, reduciendo la posibilidad de que el modelo aprenda atajos triviales y asegurando que la complejidad del aprendizaje esté alineada con el problema inductivo planteado. La “extracción” de representaciones queda delegada correctamente a la etapa de modelado, donde los embeddings son aprendidos de forma automática."
      ],
      "metadata": {
        "id": "_A_aTp_dqmIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. Conclusiones de la fase de Preparación de los Datos (CRISP-ML)"
      ],
      "metadata": {
        "id": "NDCuzfj9q2jo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La fase de preparación de los datos en este proyecto se adapta a la naturaleza relacional del problema y al objetivo inductivo planteado por los trabajos de referencia. Aunque no se aplican técnicas tradicionales de limpieza, transformación o selección de características, se ejecuta una preparación estructural deliberada del grafo, que resulta crítica para la validez experimental del modelo.\n",
        "\n",
        "La construcción de escenarios InGram y OOKB mediante particiones controladas garantiza que el modelo sea evaluado bajo condiciones realistas de generalización, evitando fugas de información y sesgos estructurales. En este sentido, la preparación de los datos no busca simplificar el problema, sino definir explícitamente las limitaciones de conocimiento bajo las cuales el modelo debe operar, es decir, información no presente durante el entrenamiento.\n",
        "\n",
        "Como resultado, esta etapa sienta las bases para un proceso de modelado sólido, reproducible y conceptualmente alineado con el objetivo del proyecto, asegurando que el desempeño observado refleje verdaderamente la capacidad inductiva del modelo."
      ],
      "metadata": {
        "id": "uoqMlcY9q4-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Environment and GPU sanity check"
      ],
      "metadata": {
        "id": "Mj6g11A-IjnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU check for future pipeline\n",
        "import torch\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA Version:\", torch.version.cuda)\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  print(\"------------No GPU. Set Runtime → Change runtime type → GPU------------\")\n",
        "\n",
        "try:\n",
        "    import torch_geometric\n",
        "    print(\"Torch Geometric:\", torch_geometric.__version__)\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Torch Geometric not found. Installing\")\n",
        "    torch_version = torch.__version__.split(\"+\")[0]\n",
        "    cuda_version = torch.version.cuda.replace(\".\", \"\")\n",
        "\n",
        "    !python -m pip install -q pyg-lib torch-scatter torch-sparse torch-cluster torch-spline-conv \\\n",
        "        -f https://data.pyg.org/whl/torch-{torch_version}+cu{cuda_version}.html\n",
        "\n",
        "    !python -m pip install -q torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eismSo1BBGoH",
        "outputId": "5ff8ee15-5c91-4e73-b810-8e7e6aa6384c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "CUDA Version: 12.6\n",
            "Torch Geometric not found. Installing\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pyg-lib (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pyg-lib\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Dataset download and normalization"
      ],
      "metadata": {
        "id": "7pI_W7KtImWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -r raw_data data"
      ],
      "metadata": {
        "id": "xFn3_3wAO-xX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Dataset download & normalization\n",
        "# =========================\n",
        "\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import pandas as pd\n",
        "from torch_geometric.datasets import WordNet18RR, FB15k_237\n",
        "\n",
        "# -------------------------\n",
        "# Paths\n",
        "# -------------------------\n",
        "RAW_DIR  = Path(\"./raw_data\")      # Raw / potentially dirty datasets\n",
        "DATA_DIR = Path(\"./data/newlinks\") # Normalized datasets (h, r, t) for New Links\n",
        "OOKB_DIR = Path(\"./data/newentities\") # Normalized datasets (h, r, t) for New Entities\n",
        "\n",
        "OOKB_DIR.mkdir(parents=True, exist_ok=True)\n",
        "RAW_DIR.mkdir(exist_ok=True)\n",
        "DATA_DIR.mkdir(parents=True,exist_ok=True)\n",
        "\n",
        "print(f\"RAW_DIR : {RAW_DIR.resolve()}\")\n",
        "print(f\"DATA_DIR: {DATA_DIR.resolve()}\")\n",
        "\n",
        "# -------------------------\n",
        "# Helpers\n",
        "# -------------------------\n",
        "def normalize_to_txt(src_path: Path, dst_path: Path):\n",
        "    \"\"\"\n",
        "    Read raw KG triple file src_path and saves first 3 columns\n",
        "    as head<TAB>rel<TAB>tail into dst_path.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(\n",
        "        src_path,\n",
        "        sep=None,\n",
        "        engine=\"python\",\n",
        "        header=None,\n",
        "        on_bad_lines=\"skip\"\n",
        "    )\n",
        "\n",
        "    if df.shape[1] < 3:\n",
        "        raise ValueError(\n",
        "            f\"[FORMAT ERROR] Invalid KG triple file: {src_path}\\n\"\n",
        "            f\"Detected columns: {df.shape[1]}\\n\"\n",
        "            \"Expected format: head, relation, tail, [optional extra columns]\"\n",
        "        )\n",
        "\n",
        "    df.iloc[:, :3].to_csv(dst_path, sep=\"\\t\", index=False, header=False)\n",
        "\n",
        "\n",
        "def pyg_dataset_to_standard(pyg_dataset, name: str):\n",
        "    \"\"\"\n",
        "    Normalize (tab) PyG raw files from raw\n",
        "    and saves as data/name/{train,valid,test}.txt\n",
        "    into data/name\n",
        "    \"\"\"\n",
        "    raw_dir = Path(pyg_dataset.raw_dir)\n",
        "    out_dir = DATA_DIR / name\n",
        "    out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    print(f\"\\nProcessing PyG dataset: {name}\")\n",
        "\n",
        "    file_map = {\n",
        "        \"train\": [\"train.txt\"],\n",
        "        \"valid\": [\"valid.txt\", \"valid.csv\"],\n",
        "        \"test\":  [\"test.txt\"]\n",
        "    }\n",
        "\n",
        "    for split, candidates in file_map.items():\n",
        "        for fname in candidates:\n",
        "            src = raw_dir / fname\n",
        "            if src.exists():\n",
        "                dst = out_dir / f\"{split}.txt\"\n",
        "                normalize_to_txt(src, dst)\n",
        "                print(f\"  -> {split}.txt\")\n",
        "                break\n",
        "        else:\n",
        "            print(f\"  [!] Missing split: {split}\")\n",
        "\n",
        "\n",
        "def download_file(url: str, dst: Path):\n",
        "    if dst.exists():\n",
        "        return\n",
        "    print(f\"Downloading {dst.name}...\")\n",
        "    r = requests.get(url)\n",
        "    r.raise_for_status()\n",
        "    dst.write_bytes(r.content)\n",
        "\n",
        "# -------------------------\n",
        "# PyG datasets\n",
        "# -------------------------\n",
        "print(\"\\n--- Downloading PyG datasets ---\")\n",
        "\n",
        "wn18rr = WordNet18RR(root=RAW_DIR / \"WordNet18RR\")\n",
        "pyg_dataset_to_standard(wn18rr, \"WN18RR\")\n",
        "\n",
        "fb237 = FB15k_237(root=RAW_DIR / \"FB15k-237\")\n",
        "pyg_dataset_to_standard(fb237, \"FB15k-237\")\n",
        "\n",
        "# -------------------------\n",
        "# External datasets\n",
        "# -------------------------\n",
        "print(\"\\n--- Downloading external datasets ---\")\n",
        "\n",
        "EXTERNAL_DATASETS = {\n",
        "    \"CoDEx-M\": \"https://raw.githubusercontent.com/tsafavi/codex/master/data/triples/codex-m/\",\n",
        "    \"WN11\":    \"https://raw.githubusercontent.com/KGCompletion/TransL/master/WN11/\",\n",
        "    \"FB13\":    \"https://raw.githubusercontent.com/KGCompletion/TransL/master/FB13/\",\n",
        "}\n",
        "\n",
        "for name, base_url in EXTERNAL_DATASETS.items():\n",
        "    raw_out = RAW_DIR / name\n",
        "    data_out = DATA_DIR / name\n",
        "    raw_out.mkdir(exist_ok=True)\n",
        "    data_out.mkdir(exist_ok=True)\n",
        "\n",
        "    print(f\"\\n{name}\")\n",
        "    for split in [\"train\", \"valid\", \"test\"]:\n",
        "        url = f\"{base_url}{split}.txt\"\n",
        "        raw_path = raw_out / f\"{split}.txt\"\n",
        "        data_path = data_out / f\"{split}.txt\"\n",
        "\n",
        "        download_file(url, raw_path)\n",
        "        normalize_to_txt(raw_path, data_path)\n",
        "        print(f\"  -> {split}.txt\")\n",
        "\n",
        "    if name != \"CoDEx-M\":\n",
        "      for split in [\"entity2id\", \"relation2id\"]:\n",
        "          url = f\"{base_url}{split}.txt\"\n",
        "          raw_path = raw_out / f\"{split}.txt\"\n",
        "          download_file(url, raw_path)\n",
        "\n",
        "print(\"\\n[DONE] All datasets downloaded and normalized.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4frQnX-Buym",
        "outputId": "de72160a-d1fd-4bbc-ca8f-83c1733ca303"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAW_DIR : /content/raw_data\n",
            "DATA_DIR: /content/data/newlinks\n",
            "\n",
            "--- Downloading PyG datasets ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/WN18RR/original/train.txt\n",
            "Downloading https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/WN18RR/original/valid.txt\n",
            "Downloading https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/WN18RR/original/test.txt\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing PyG dataset: WN18RR\n",
            "  -> train.txt\n",
            "  -> valid.txt\n",
            "  -> test.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/FB15k-237/train.txt\n",
            "Downloading https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/FB15k-237/valid.txt\n",
            "Downloading https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/FB15k-237/test.txt\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing PyG dataset: FB15k-237\n",
            "  -> train.txt\n",
            "  -> valid.txt\n",
            "  -> test.txt\n",
            "\n",
            "--- Downloading external datasets ---\n",
            "\n",
            "CoDEx-M\n",
            "Downloading train.txt...\n",
            "  -> train.txt\n",
            "Downloading valid.txt...\n",
            "  -> valid.txt\n",
            "Downloading test.txt...\n",
            "  -> test.txt\n",
            "\n",
            "WN11\n",
            "Downloading train.txt...\n",
            "  -> train.txt\n",
            "Downloading valid.txt...\n",
            "  -> valid.txt\n",
            "Downloading test.txt...\n",
            "  -> test.txt\n",
            "Downloading entity2id.txt...\n",
            "Downloading relation2id.txt...\n",
            "\n",
            "FB13\n",
            "Downloading train.txt...\n",
            "  -> train.txt\n",
            "Downloading valid.txt...\n",
            "  -> valid.txt\n",
            "Downloading test.txt...\n",
            "  -> test.txt\n",
            "Downloading entity2id.txt...\n",
            "Downloading relation2id.txt...\n",
            "\n",
            "[DONE] All datasets downloaded and normalized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. New Link splits (InGram setup)"
      ],
      "metadata": {
        "id": "zjt1eS4UIpv0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3PKZGpjj0VUN"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Inductive relation-based splits (NL-*)\n",
        "# =========================\n",
        "\n",
        "from pathlib import Path\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "SEED = 42\n",
        "\n",
        "# Defined as in paper, scenarios for different persentages of\n",
        "# dataset links used as unseen in training\n",
        "ALPHAS = {\n",
        "    \"NL-25\": 0.25,\n",
        "    \"NL-50\": 0.50,\n",
        "    \"NL-75\": 0.75,\n",
        "    \"NL-100\": 1.00,\n",
        "}\n",
        "\n",
        "# Reproducibility\n",
        "random.seed(SEED)\n",
        "\n",
        "# -------------------------\n",
        "# IO helpers\n",
        "# -------------------------\n",
        "def read_triples(path: Path):\n",
        "    triples = []\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            h, r, t = line.rstrip(\"\\n\").split(\"\\t\")\n",
        "            triples.append((h, r, t))\n",
        "    return triples\n",
        "\n",
        "\n",
        "def write_triples(path: Path, triples):\n",
        "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        for h, r, t in triples:\n",
        "            f.write(f\"{h}\\t{r}\\t{t}\\n\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Core logic\n",
        "# -------------------------\n",
        "def generate_inductive_splits(dataset_dir: Path):\n",
        "    \"\"\"\n",
        "    Generate inductive relation-based splits (NL-*) for a dataset directory.\n",
        "\n",
        "    The input directory must contain:\n",
        "        train.txt\n",
        "        valid.txt\n",
        "        test.txt\n",
        "\n",
        "    The function creates, inside the same directory:\n",
        "        NL-25/, NL-50/, NL-75/, NL-100/\n",
        "    each containing train/valid/test splits where relations in valid/test\n",
        "    are completely unseen during training.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset_dir : Path\n",
        "        Path to a dataset directory under BASE_DATA_DIR.\n",
        "    \"\"\"\n",
        "    train_path = dataset_dir / \"train.txt\"\n",
        "    valid_path = dataset_dir / \"valid.txt\"\n",
        "    test_path  = dataset_dir / \"test.txt\"\n",
        "\n",
        "    if not (train_path.exists() and valid_path.exists() and test_path.exists()):\n",
        "        print(f\"[SKIP] {dataset_dir.name}: missing train/valid/test files\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n[DATASET] {dataset_dir.name}\")\n",
        "\n",
        "    train = read_triples(train_path)\n",
        "    valid = read_triples(valid_path)\n",
        "    test  = read_triples(test_path)\n",
        "\n",
        "    # All triples of dataset\n",
        "    all_triples = train + valid + test\n",
        "\n",
        "    # Group triples by relation\n",
        "    rel2triples = defaultdict(list)\n",
        "    for h, r, t in all_triples:\n",
        "        rel2triples[r].append((h, r, t))\n",
        "\n",
        "    # Relations\n",
        "    relations = list(rel2triples.keys())\n",
        "    num_relations = len(relations)\n",
        "\n",
        "    print(f\"  Total relations : {num_relations}\")\n",
        "    print(f\"  Total triples   : {len(all_triples)}\")\n",
        "\n",
        "    for split_name, alpha in ALPHAS.items():\n",
        "        # New = unseen at training triples\n",
        "        # number of new triples\n",
        "        n_new = int(round(num_relations * alpha))\n",
        "\n",
        "        # Randomly selected\n",
        "        shuffled = relations[:]\n",
        "        random.shuffle(shuffled)\n",
        "\n",
        "        # new links -> val/test\n",
        "        # old links -> train\n",
        "        new_rels = set(shuffled[:n_new])\n",
        "        old_rels = set(shuffled[n_new:])\n",
        "\n",
        "        # old links -> train\n",
        "        train_split = []\n",
        "        for r in old_rels:\n",
        "            train_split.extend(rel2triples[r])\n",
        "\n",
        "        # new links -> val/test\n",
        "        new_triples = []\n",
        "        for r in new_rels:\n",
        "            new_triples.extend(rel2triples[r])\n",
        "\n",
        "        # val/test -> 50%/50% of new links total\n",
        "        random.shuffle(new_triples)\n",
        "        mid = len(new_triples) // 2\n",
        "        valid_split = new_triples[:mid]\n",
        "        test_split  = new_triples[mid:]\n",
        "\n",
        "        # Safety checks\n",
        "        assert {r for _, r, _ in train_split}.isdisjoint(new_rels)\n",
        "        assert {r for _, r, _ in valid_split}.issubset(new_rels)\n",
        "        assert {r for _, r, _ in test_split}.issubset(new_rels)\n",
        "\n",
        "        out_dir = dataset_dir / split_name\n",
        "        out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        write_triples(out_dir / \"train.txt\", train_split)\n",
        "        write_triples(out_dir / \"valid.txt\", valid_split)\n",
        "        write_triples(out_dir / \"test.txt\",  test_split)\n",
        "\n",
        "        print(\n",
        "            f\"  [{split_name}] \"\n",
        "            f\"new_rel={len(new_rels)} | \"\n",
        "            f\"train={len(train_split)} | \"\n",
        "            f\"valid={len(valid_split)} | \"\n",
        "            f\"test={len(test_split)}\"\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Run for all datasets\n",
        "# -------------------------\n",
        "print(\"\\n=== Generating inductive splits for all datasets ===\")\n",
        "\n",
        "for dataset_dir in DATA_DIR.iterdir():\n",
        "    if dataset_dir.is_dir():\n",
        "        generate_inductive_splits(dataset_dir)\n",
        "\n",
        "print(\"\\n[DONE] All NL-* splits generated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b5Rk6RiTudv",
        "outputId": "49199bd9-ed03-408e-c778-522c38a15a59"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Generating inductive splits for all datasets ===\n",
            "\n",
            "[DATASET] CoDEx-M\n",
            "  Total relations : 51\n",
            "  Total triples   : 206205\n",
            "  [NL-25] new_rel=13 | train=173839 | valid=16183 | test=16183\n",
            "  [NL-50] new_rel=26 | train=137278 | valid=34463 | test=34464\n",
            "  [NL-75] new_rel=38 | train=97895 | valid=54155 | test=54155\n",
            "  [NL-100] new_rel=51 | train=0 | valid=103102 | test=103103\n",
            "\n",
            "[DATASET] FB15k-237\n",
            "  Total relations : 237\n",
            "  Total triples   : 310116\n",
            "  [NL-25] new_rel=59 | train=220712 | valid=44702 | test=44702\n",
            "  [NL-50] new_rel=118 | train=126564 | valid=91776 | test=91776\n",
            "  [NL-75] new_rel=178 | train=75589 | valid=117263 | test=117264\n",
            "  [NL-100] new_rel=237 | train=0 | valid=155058 | test=155058\n",
            "\n",
            "[DATASET] FB13\n",
            "  Total relations : 13\n",
            "  Total triples   : 375514\n",
            "  [NL-25] new_rel=3 | train=317509 | valid=29002 | test=29003\n",
            "  [NL-50] new_rel=6 | train=159199 | valid=108157 | test=108158\n",
            "  [NL-75] new_rel=10 | train=135632 | valid=119941 | test=119941\n",
            "  [NL-100] new_rel=13 | train=0 | valid=187757 | test=187757\n",
            "\n",
            "[DATASET] WN18RR\n",
            "  Total relations : 11\n",
            "  Total triples   : 93003\n",
            "  [NL-25] new_rel=3 | train=52533 | valid=20235 | test=20235\n",
            "  [NL-50] new_rel=6 | train=46366 | valid=23318 | test=23319\n",
            "  [NL-75] new_rel=8 | train=5406 | valid=43798 | test=43799\n",
            "  [NL-100] new_rel=11 | train=0 | valid=46501 | test=46502\n",
            "\n",
            "[DATASET] WN11\n",
            "  Total relations : 11\n",
            "  Total triples   : 138887\n",
            "  [NL-25] new_rel=3 | train=55644 | valid=41621 | test=41622\n",
            "  [NL-50] new_rel=6 | train=17247 | valid=60820 | test=60820\n",
            "  [NL-75] new_rel=8 | train=49919 | valid=44484 | test=44484\n",
            "  [NL-100] new_rel=11 | train=0 | valid=69443 | test=69444\n",
            "\n",
            "[DONE] All NL-* splits generated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 New Entities splits (OOKB setup)"
      ],
      "metadata": {
        "id": "CC-w0V9Aj21A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "UNSEEN_RATIO = 0.20   # % de entidades OOKB\n",
        "\n",
        "# -------------------------\n",
        "# OOKB logic\n",
        "# -------------------------\n",
        "def generate_ookb_splits(dataset_dir: Path):\n",
        "    train_path = dataset_dir / \"train.txt\"\n",
        "    valid_path = dataset_dir / \"valid.txt\"\n",
        "    test_path  = dataset_dir / \"test.txt\"\n",
        "\n",
        "    if not (train_path.exists() and valid_path.exists() and test_path.exists()):\n",
        "        print(f\"[SKIP] {dataset_dir.name}: missing train/valid/test\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n[OOKB DATASET] {dataset_dir.name}\")\n",
        "\n",
        "    train = read_triples(train_path)\n",
        "    valid = read_triples(valid_path)\n",
        "    test  = read_triples(test_path)\n",
        "\n",
        "    # All triples of dataset\n",
        "    all_triples = train + valid + test\n",
        "\n",
        "    # Collect entities & relations\n",
        "    entities = set()\n",
        "    relations = set()\n",
        "    for h, r, t in all_triples:\n",
        "        entities.update([h, t])\n",
        "        relations.add(r)\n",
        "\n",
        "    entities  = list(entities)\n",
        "    relations = list(relations)\n",
        "\n",
        "    # Select unseen entities\n",
        "    random.shuffle(entities)\n",
        "    # Ratio-based selection\n",
        "    n_unseen = int(round(len(entities) * UNSEEN_RATIO))\n",
        "    unseen_entities = set(entities[:n_unseen])\n",
        "\n",
        "    print(f\"  entities={len(entities)} | unseen={len(unseen_entities)}\")\n",
        "\n",
        "    # Split triples\n",
        "    train_split = []\n",
        "    new_triples = []\n",
        "\n",
        "    # new triples = triples not seen at training\n",
        "    # train set directly assigned\n",
        "    for h, r, t in all_triples:\n",
        "        if h in unseen_entities or t in unseen_entities:\n",
        "            new_triples.append((h, r, t))\n",
        "        else:\n",
        "            train_split.append((h, r, t))\n",
        "\n",
        "    random.shuffle(new_triples)\n",
        "    mid = len(new_triples) // 2\n",
        "\n",
        "    # val/test -> 50%/50% of new links total\n",
        "    valid_split = new_triples[:mid]\n",
        "    test_split  = new_triples[mid:]\n",
        "\n",
        "    # -------- safety checks --------\n",
        "    assert all(\n",
        "        h not in unseen_entities and t not in unseen_entities\n",
        "        for h, _, t in train_split\n",
        "    )\n",
        "\n",
        "    assert any(\n",
        "        h in unseen_entities or t in unseen_entities\n",
        "        for h, _, t in valid_split + test_split\n",
        "    )\n",
        "\n",
        "    # -------- output --------\n",
        "    out_dir = OOKB_DIR / dataset_dir.name\n",
        "    out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    write_triples(out_dir / \"train.txt\", train_split)\n",
        "    write_triples(out_dir / \"valid.txt\", valid_split)\n",
        "    write_triples(out_dir / \"test.txt\",  test_split)\n",
        "\n",
        "    # In order to replicate OOKB structure\n",
        "    # dictionaries need to be generated\n",
        "    # -------- dictionaries --------\n",
        "    entity2id = {e: i for i, e in enumerate(sorted(entities))}\n",
        "    relation2id = {r: i for i, r in enumerate(sorted(relations))}\n",
        "    unseenentity2id = {e: entity2id[e] for e in sorted(unseen_entities)}\n",
        "\n",
        "    with (out_dir / \"entity2id.txt\").open(\"w\") as f:\n",
        "        for e, i in entity2id.items():\n",
        "            f.write(f\"{e}\\t{i}\\n\")\n",
        "\n",
        "    with (out_dir / \"relation2id.txt\").open(\"w\") as f:\n",
        "        for r, i in relation2id.items():\n",
        "            f.write(f\"{r}\\t{i}\\n\")\n",
        "\n",
        "    # Unseen dict is intended to be used as a reference\n",
        "    # to perform semantic or text verification after predictions\n",
        "    with (out_dir / \"unseenentity2id.txt\").open(\"w\") as f:\n",
        "        for e, i in unseenentity2id.items():\n",
        "            f.write(f\"{e}\\t{i}\\n\")\n",
        "\n",
        "    print(\n",
        "        f\"  train={len(train_split)} | \"\n",
        "        f\"valid={len(valid_split)} | \"\n",
        "        f\"test={len(test_split)}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "XnXvTAMhc9Uo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Run OOKB for all datasets\n",
        "# -------------------------\n",
        "\n",
        "import shutil\n",
        "\n",
        "OOKB_PREDEFINED = {\"WN11\", \"FB13\"}\n",
        "\n",
        "# Predefined datasets already meet OOKB requirements\n",
        "print(\"\\n=== Preparing PREDEFINED OOKB datasets (from RAW) ===\")\n",
        "\n",
        "for name in OOKB_PREDEFINED:\n",
        "    src = RAW_DIR / name\n",
        "    dst = OOKB_DIR / name\n",
        "\n",
        "    if not src.exists():\n",
        "        print(f\"[SKIP] {name}: not found in RAW_DIR\")\n",
        "        continue\n",
        "\n",
        "    if dst.exists():\n",
        "        print(f\"[OK] {name}: already exists\")\n",
        "        continue\n",
        "\n",
        "    shutil.copytree(src, dst)\n",
        "    print(f\"[COPIED] {name}\")\n",
        "\n",
        "\n",
        "# Custom datasets need to be prepared explicitly\n",
        "print(\"\\n=== Generating OOKB splits (custom datasets only) ===\")\n",
        "\n",
        "for dataset_dir in DATA_DIR.iterdir():\n",
        "    if not dataset_dir.is_dir():\n",
        "        continue\n",
        "\n",
        "    if dataset_dir.name in OOKB_PREDEFINED:\n",
        "        continue   # WN11 / FB13 ya están listos\n",
        "\n",
        "    generate_ookb_splits(dataset_dir)\n",
        "\n",
        "print(\"\\n[DONE] All OOKB datasets generated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "266aIXRVxAuh",
        "outputId": "e1ac6183-5bc3-41d6-b9a1-4e9e06cc4d80"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Preparing PREDEFINED OOKB datasets (from RAW) ===\n",
            "[COPIED] FB13\n",
            "[COPIED] WN11\n",
            "\n",
            "=== Generating OOKB splits (custom datasets only) ===\n",
            "\n",
            "[OOKB DATASET] CoDEx-M\n",
            "  entities=17050 | unseen=3410\n",
            "  train=138242 | valid=33981 | test=33982\n",
            "\n",
            "[OOKB DATASET] FB15k-237\n",
            "  entities=14541 | unseen=2908\n",
            "  train=203159 | valid=53478 | test=53479\n",
            "\n",
            "[OOKB DATASET] WN18RR\n",
            "  entities=40943 | unseen=8189\n",
            "  train=60305 | valid=16349 | test=16349\n",
            "\n",
            "[DONE] All OOKB datasets generated.\n"
          ]
        }
      ]
    }
  ]
}